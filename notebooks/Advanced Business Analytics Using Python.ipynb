{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = red>Machine Learning for Business Analytics:<br>A Deep Dive into Data with Python</font>\n",
    "=======\n",
    "<br>\n",
    "    <center><img src=\"http://dataanalyticscorp.com/wp-content/uploads/2018/03/logo.png\"></center>\n",
    "<br>\n",
    "Taught by: \n",
    "\n",
    "* Walter R. Paczkowski, Ph.D. \n",
    "\n",
    "    * My Affliations: [Data Analytics Corp.](http://www.dataanalyticscorp.com/) and [Rutgers University](https://economics.rutgers.edu/people/teaching-personnel)\n",
    "    * [Email Me With Questions](mailto:walt@dataanalyticscorp.com)\n",
    "    * [Learn About Me](http://www.dataanalyticscorp.com/)\n",
    "    * [See My LinkedIn Profile](https://www.linkedin.com/in/walter-paczkowski-a17a1511/)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents\n",
    "-------------\n",
    "\n",
    "1. [About this Notebook](#About-this-Notebook)\n",
    "2. [Helpful Online Tutorials](#Helpful-Online-Tutorials)\n",
    "3. [Helpful/Must-Read Book](#Helpful/Must-Read-Book)\n",
    "3. [**_Lesson 0: Preliminary Stuff_**](#Lesson-0:-Preliminary-Stuff)\n",
    "    1. [Load Python Packages](#Load-Python-Packages)\n",
    "    - [Data and Slide Paths](#Data-and-Slide-Paths)\n",
    "4. [**_Lesson I: Introduction_**](#Lesson-I:-Introduction)\n",
    "    1. [Case Study Data Dictionary](#Case-Study-Data-Dictionary)\n",
    "    2. [Load Case Study Data](#Load-Case-Study-Data)\n",
    "5. [**_Lesson II: Data Preprocessing_**](#Lesson-II:-Data-Preprocessing)\n",
    "    1. [Preprocessing Step I: Transformation](#Preprocessing-Step-I:-Transformation)\n",
    "    - [Preprocessing Step II: Encoding](#Preprocessing-Step-II:-Encoding)\n",
    "    - [Preprocessing Step III: Dimension Reduction](#Preprocessing-Step-III:-Dimension-Reduction)\n",
    "    2. [Exercises II.A](#Exercises-II.A)\n",
    "        1. [Exercise II.A.1](#Exercise-II.A.1)\n",
    "        - [Exercise II.A.2](#Exercise-II.A.2)\n",
    "        - [Exercise II.A.3](#Exercise-II.A.3)\n",
    "        - [Exercise II.A.4](#Exercise-II.A.4)\n",
    "- [**_Lesson III: Supervised Learning Methods_**](#Lesson-III:-Supervised-Learning-Methods)\n",
    "    1. [Supervised vs. Unsupervised Learning](#Supervised-vs.-Unsupervised-Learning)\n",
    "    - [Generalized Linear Model (GLM)](#Generalized-Linear-Model-(GLM))\n",
    "        1. [Train/Test Split Data](#Train/Test-Split-Data)\n",
    "        - [Exercsies III.A](#Exercises-III.A)\n",
    "            1. [Exercise III.A.1](#Exercise-III.A.1)\n",
    "            - [Exercise III.A.2](#Exercise-III.A.2)\n",
    "        - [Linear Models: Identity Link](#Linear-Models:-Identity-Link)\n",
    "        - [Exercsies III.B](#Exercises-III.B)\n",
    "            1. [Exercise III.B.1](#Exercise-III.B.1)\n",
    "        - [Logistic Regression: Logit Link](#Logistic-Regression:-Logit-Link)\n",
    "        - [Exercsies III.C](#Exercises-III.C)\n",
    "            1. [Exercise III.C.1](#Exercise-III.C.1)\n",
    "            - [Exercise III.C.2](#Exercise-III.C.2)\n",
    "            - [Exercise III.C.3](#Exercise-III.C.3)\n",
    "    - [Classification](#Classification)\n",
    "        1. [Naive Bayes](#Naive-Bayes)\n",
    "        - [Support Vector Machines](#Support-Vector-Machines)\n",
    "        - [Decision Trees](#Decision-Trees)\n",
    "- [**_Lesson IV: Unsupervised Learning Methods_**](#Lesson-IV:-Unsupervised-Learning-Methods)\n",
    "    1. [Types and Characteristics of Unsupervised Learning](#Types-and-Characteristics-of-Unsupervised-Learning)\n",
    "    - [Clustering](#Clustering)\n",
    "        1. [Hierarchical](#Hierarchical)\n",
    "        - [Exercsies IV.A](#Exercises-IV.A)\n",
    "            1. [Exercise IV.A.1](#Exercise-IV.A.1)        \n",
    "        - [K-Means](#K-Means)\n",
    "        - [Mixture Models](#Mixture-Models)\n",
    "        - [Gaussian Mixture Models](#Gaussian-Mixture-Models)\n",
    "- [**_Lesson V: Model Evaluation_**](#Lesson-V:-Model-Evaluation)\n",
    "- [Contact Information](#Contact-Information)\n",
    "- [Exercise Solutions](#Exercise-Solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About this Notebook\n",
    "-----------------------------\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "This notebook accompanies the PDF presentation **_Machine Learning for Business Analytics: A Deep Dive into Data with Python_** by Walter R. Paczkowski, Ph.D. (2019).  There is more content and commentary in this notebook than in the presentation deck.  Nonetheless, the two complement each other and so should be studied together.  Every effort has been made to use the same key slide titles in the presentation deck and this notebook to help your learning.\n",
    "\n",
    "For your convenience, the key slides from the PDF presentation have been incorporated into this notebook so everything is sourced in one place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful Online Tutorials\n",
    "---------------------------------\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "* <a href=\"http://docs.python.org/2/tutorial/\" target=\"_parent\">Python Tutorial</a>\n",
    "\n",
    "* <a href=\"https://pandas.pydata.org/pandas-docs/stable/getting_started/tutorials.html\" target=\"_parent\">Pandas Tutorial</a>\n",
    "\n",
    "* <a href=\"https://seaborn.pydata.org/tutorial.html\" target=\"_parent\">Seaborn Tutorial</a>\n",
    "\n",
    "* <a href=\"https://www.statsmodels.org/stable/index.html\" target=\"_parent\">Statsmodels Tutorial</a>\n",
    "\n",
    "* <a href=\"https://scikit-learn.org/stable/tutorial/index.html\" target=\"_parent\">scikit-learn Tutorials</a>\n",
    "\n",
    "\n",
    "Helpful/Must-Read Book\n",
    "-----------------------------------\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "* <a href=\"https://www.amazon.com/gp/product/1491957662/ref=as_li_tl?ie=UTF8&tag=quantpytho-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1491957662&linkId=8c3bf87b221dbcd8f541f0db20d4da83\" target=\"_parent\">Main Pandas go-to book: </a> *Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython* (2nd Edition) by Wes McKinney.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson 0: Preliminary Stuff\n",
    "---------------------------------------\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "You have to load a Python package before you can use it.  Loading is done using an *import* command.  An alias is assigned when you import the package.  I recommend loading all the packages at once at the beginning of your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Python Packages\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Data Management <===\n",
    "##\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "##\n",
    "## ===> Visualization <===\n",
    "##\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "##\n",
    "## ===> Analytical <===\n",
    "##\n",
    "## sklearn packages:\n",
    "##   1. preprocessing\n",
    "##   2. principal components\n",
    "##   3. MinMaxScaler\n",
    "##   4. standardScaler\n",
    "##\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "##\n",
    "## Train_test_split\n",
    "##\n",
    "from sklearn.model_selection import train_test_split\n",
    "##\n",
    "## Modeling\n",
    "##   Notice the new import command for\n",
    "##   the formula API and the summary option\n",
    "##\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf \n",
    "from statsmodels.compat import lzip\n",
    "##\n",
    "## Confusion matrix functions\n",
    "##\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "##\n",
    "## r2_score function from the sklearn metrics package\n",
    "##\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "##\n",
    "## Decision tree classifier\n",
    "##\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_graphviz\n",
    "##\n",
    "## Import needed packages\n",
    "##\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "##\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "##\n",
    "## Clustering \n",
    "##\n",
    "##   = Hierarchical\n",
    "##\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "##\n",
    "##    = KMeans Clustering\n",
    "##\n",
    "from sklearn.cluster import KMeans\n",
    "##\n",
    "##    = Gaussian Mixture Model\n",
    "##\n",
    "from sklearn.mixture import GaussianMixture \n",
    "##\n",
    "## Image for displaying slides\n",
    "##\n",
    "from IPython.display import Image\n",
    "##\n",
    "## Gaussian Naive Bayes\n",
    "##\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "##\n",
    "## SVM\n",
    "##\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Slide Paths\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "It is best practice to define paths in one location.  This makes error finding and changes easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## slide code\n",
    "##\n",
    "from IPython.display import Image\n",
    "def slide(what):\n",
    "    display( Image( \"../Slides/ABA_Page_\" + what + \".jpg\", width = 100, height = 100, retina = True ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Set data and slides paths\n",
    "##\n",
    "path_data = r'../Data/furniture/final data files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson I: Introduction\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '002' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '003' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '004' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '005' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '006' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '007' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '008' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '009' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '010' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '011' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study Data Dictionary\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "| Variable                  | Values                                 | Source       | Mnemonic     |\n",
    "|---------------------------|----------------------------------------|--------------|--------------|\n",
    "| Order Number              | Nominal Integer                        | Order Sys    | Onum         |\n",
    "| Customer ID               | Nominal                                | Customer Sys | CID          | \n",
    "| Transaction Date          | MM/DD/YYYY                             | Order Sys    | Tdate        | \n",
    "| Product Line ID           | Five rooms of house                    | Product Sys  | Pline        |\n",
    "| Product Class ID          | Item in line                           | Product Sys  | Pclass       |\n",
    "| Units Sold                | Number of units per order              | Order Sys    | Usales       |\n",
    "| Product Returned?         | Yes/No                                 | Order Sys    | Return       |\n",
    "| Amount Returned           | Number of units                        | Order Sys    | returnAmount |\n",
    "| Material Cost/Unit        | \\$US cost of material                  | Product Sys  | Mcost        |\n",
    "| List Price                | \\$US list                              | Price Sys    | Lprice       |\n",
    "| Dealer Discount           | \\% discount to dealer (decimal)        | Sales Sys    | Ddisc        |\n",
    "| Competitive Discount      | \\% discount for competition (decimal)  | Sales Sys    | Cdisc        |\n",
    "| Order Size Discount       | \\% discount for size (decimal)         | Sales Sys    | Odisc        |\n",
    "| Customer Pickup Allowance | \\% discount for pickup (decimal)       | Sales Sys    | Pdisc        |\n",
    "| Total Discount            | \\% discount                            | Calculated: Sum of discounts | Tdisc         |\n",
    "| Pocket Price              | \\$US                                   | Calculated: LPrice $\\times$ (1 - TDisc) | Pprice  | \n",
    "| Log of Unit Sales         | Log sales                              | Calculated: log(Usales)  | log_Usales  |\n",
    "| Log of Pocket Price       | \\$US                                   | Calculated: log(Pprice)  | log_Pprice  |\n",
    "| Revenue                   | \\$US                                   | Calculated: Usales $\\times$ Pprice | Rev          |\n",
    "| Contribution              | \\$US                                   | Calculated: Rev - Mcost | Con  |\n",
    "| Contribution Margin       | \\%                                     | Calculated: Con/Rev | CM |\n",
    "| Net Revenue               | \\$US                                   | Calculated: (Usales - returnAmount) $\\times$  Pprice  | netRev  |\n",
    "| Lost Revenue         |  \\$US   | Calculated: Rev - netRev  | lostRev  | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Case Study Data\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import the data.  The parse_dates argument says to \n",
    "## treat Tdate as a date object.\n",
    "##\n",
    "file = r'orders.csv'\n",
    "df_orders = pd.read_csv( path_data + file, parse_dates = [ 'Tdate' ] )\n",
    "pd.set_option('display.max_columns', 8)\n",
    "##\n",
    "## Add quarter variable to the DataFrame\n",
    "##\n",
    "df_orders[ 'Quarter' ] = df_orders.Tdate.dt.quarter\n",
    "data = { 1: 'Q1', 2: 'Q2', 3: 'Q3', 4: 'Q4' }\n",
    "df_orders[ 'Qtr' ] = df_orders[ 'Quarter' ].map( data )\n",
    "##\n",
    "## Initial Calculations\n",
    "##\n",
    "x = [ 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "df_orders[ 'Tdisc' ] = df_orders[ x ].sum( axis = 1 )\n",
    "##\n",
    "df_orders[ 'Pprice' ] = df_orders.Lprice*( 1 - df_orders.Tdisc )\n",
    "##\n",
    "df_orders[ 'Rev' ] = df_orders.Usales * df_orders.Pprice\n",
    "##\n",
    "df_orders[ 'Con' ] = df_orders.Rev - df_orders.Mcost\n",
    "df_orders[ 'CM' ] = df_orders.Con/df_orders.Rev\n",
    "##\n",
    "df_orders[ 'netRev' ] = ( df_orders.Usales - df_orders.returnAmount )*df_orders.Pprice\n",
    "df_orders[ 'lostRev' ] = df_orders.Rev - df_orders.netRev\n",
    "##\n",
    "## Import a second DataFrame on the customers\n",
    "##\n",
    "file = r'customers.csv'\n",
    "df_cust = pd.read_csv( path_data + file )\n",
    "##\n",
    "## Do an inner join using CID as the link\n",
    "##\n",
    "df = pd.merge( df_orders, df_cust, on = 'CID' )\n",
    "##\n",
    "## Display shape and head of the DataFrame\n",
    "##\n",
    "print( 'Shape of the DataFrame: {}'.format( df.shape ) )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson II: Data Preprocessing\n",
    "------------------------------------------\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '013' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '014' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '014' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '015' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Step I: Transformation\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot unit sales histogram\n",
    "##\n",
    "x = df.Usales\n",
    "ax = sns.distplot( x )\n",
    "ax.set( title = 'Histogram of Unit Sales', xlabel = 'Unit Sales', ylabel = 'Proportion' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Print mean and standard deviation of unit sales\n",
    "##\n",
    "print( 'Mean of Unit Sales: {0:0.3f}\\nStandard Deviation of Unit Sales: {1:0.3f}'.format( x.mean(), x.std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Standardize unit sales\n",
    "## Use preprocessor -- see package loading section\n",
    "##\n",
    "x_standard = pp.scale( x )\n",
    "print( 'Mean of Standardized Unit Sales: {0:0.3f}\\nStandard Deviation of Standardized Unit Sales: {1:0.3f}'.\n",
    "      format( x_standard.mean(), x_standard.std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot histogram of standardized unit sales\n",
    "##\n",
    "ax = sns.distplot( x_standard )\n",
    "ax.set( title = 'Histogram of Standardized Unit Sales', xlabel = 'Unit Sales', ylabel = 'Proportion' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice that the x-axis scale differs from the previous histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Transform to a [0, 1] range using the MinMaxScaler\n",
    "##\n",
    "tmp = df[ ['Usales' ] ]\n",
    "scaler = MinMaxScaler()\n",
    "tmp[ 'Usales_minMax_scaled' ] = scaler.fit_transform( tmp )\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Step II: Encoding\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '027' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '028' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '029' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '030' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '031' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '032' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## One-hot encoding\n",
    "##\n",
    "enc = pp.OneHotEncoder()  ## Order is alphanumeric so Midwest is first\n",
    "tmp = df[ ['Region' ] ]\n",
    "ohe = enc.fit_transform( tmp ).toarray()\n",
    "print( 'One-Hot Encoded Array:\\n{}'.format( ohe ) )\n",
    "## \n",
    "## Create a DataFrame for easier viewing\n",
    "##\n",
    "df_tmp = pd.DataFrame(ohe, columns = [\"Region_\" + str( int( i ) ) for i in range( ohe.shape[ 1 ] ) ] )\n",
    "df_ohe = pd.concat( [ df.Region, df_tmp ], axis=1 )\n",
    "df_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Step III: Dimension Reduction\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '035' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '036' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Subset features for dimension reduction\n",
    "##\n",
    "x = [ 'Odisc', 'Cdisc', 'Ddisc', 'Pdisc' ]\n",
    "x_standard = df[ x ]\n",
    "##\n",
    "## Drop NaN since some discounts have missing values\n",
    "##\n",
    "x_standard = x_standard.dropna( )\n",
    "##\n",
    "## View the head\n",
    "##\n",
    "print( x_standard.head() )\n",
    "y_standard = df[ [ 'Usales' ] ]\n",
    "x_standard = StandardScaler().fit_transform( x_standard )\n",
    "df_x_standard = pd.DataFrame( x_standard )\n",
    "df_x_standard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Find principal components\n",
    "##\n",
    "pca = PCA( n_components = 4 )\n",
    "principalComponents = pca.fit_transform( x_standard )\n",
    "##\n",
    "## Extract all four components\n",
    "##\n",
    "principalDf = pd.DataFrame( data = principalComponents,\n",
    "             columns = [ 'principal component 1', 'principal component 2',\n",
    "                         'principal component 3', 'principal component 4' ] )\n",
    "##\n",
    "## Concatenate with Sales data\n",
    "##\n",
    "df_pca = pd.concat( [ principalDf, df[ [ 'Usales' ] ] ], axis = 1 )\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Explained variance\n",
    "##\n",
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "These are the variance components accounted for by each principal component, in descending order.  The sum of these variance components is the total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Explained variance: proportion\n",
    "##\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "These are the proportion of total data variance accounted for by each principal component, in descending order.  They sum to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Singular values\n",
    "##\n",
    "pca.singular_values_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The singular values are used to calculate the variance components.  See the interpretation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Summary Table\n",
    "##\n",
    "data = { 'Singular Value':pca.singular_values_, \n",
    "         'Variance':pca.explained_variance_,\n",
    "         '%Variance':pca.explained_variance_ratio_*100 }\n",
    "var_report = pd.DataFrame( data )\n",
    "var_report[ 'Cum Sum' ] = var_report[ '%Variance' ].cumsum()\n",
    "print( 'Number of observations: n = {}'.format( pca.n_samples_ ) )\n",
    "var_report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The *Variance* is the contribution to the total variance of the data and is based on the singular values: $Variance = \\frac{SV^2}{n - 1}$.  The sum of the *Variance* contribution is the total variance.  The first principal component accounts for 25.1% of the variance and the first two account for 50.2%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercises II.A\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "The HR department of a major software company is concerned about a high attrition rate (about 15% each year) among its talented work force.  The sample size for this study is $n = 4410$.  \n",
    "\n",
    "The basis for this problem can be found [here](https://www.kaggle.com/vjchoudhary7/hr-analytics-case-study/activity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise II.A.1 \n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Import the employee data and print the first five records (i.e., the \"head\").  The data are in the HR directory in a CSV file named *employees.csv*.  Call the imported data *df_hr* for consistency with later work.\n",
    "\n",
    "[See Solution](#Solution-II.A.1)\n",
    "\n",
    "The Data Dictionary is:\n",
    "\n",
    "| Variable                  | Values                                       | Source | Mnemonic         |\n",
    "|---------------------------|----------------------------------------------|--------|------------------|\n",
    "| Employee Age              | Nominal Integer                              | HR     | Age              |\n",
    "| Left Company              | Yes/No                                       | HR     | Attrition        | \n",
    "| Amount of Busines Travel  | Non-Travel/Travel_Frequently/Travel_Rarely   | HR     | BusinessTravel   |\n",
    "| Department                | Human Resources/Research & Development/Sales | HR     | Department       |\n",
    "| Distance from Home to Work| Miles                                        | HR     | DistanceFromHome |\n",
    "| Education Level           | Indicator Variable                           | HR     | Education        |\n",
    "| Education Major           | Six Majors as Categorical                    | HR     | EducationField   |\n",
    "| Employee Count            | Just 1 For All Employees                     | HR     | EmployeeCount    |\n",
    "| Employee ID Number        | Nominal Interger                             | HR     | EmployeeID       |\n",
    "| Gender                    | Male/Female                                  | HR     | Gender           |\n",
    "| Job Level                 | Nominal Integer 1 - 5                        | HR     | JobLevel         |\n",
    "| Job Role                  | Nine Categorical Levels                      | HR     | JobRole          |\n",
    "| Martial Status            | Divorced/Married/Single                      | HR     | MaritalStatus    |\n",
    "| Monthly Income            | US\\$                                         | HR     | MonthlyIncome    |\n",
    "| Number of Companies Worked Before| Intger Values (0 = None Before)       | HR     | NumCompaniesWorked |\n",
    "| Over 18 Years Old?        | Y = Yes for All Employees                    | HR     | Over18           |\n",
    "| Percent Salary Increase   | Continuous Whole Number                      | HR     | PercentSalaryHike |\n",
    "| Standard Hours Work/Day   | 8 for All Employees                          | HR     | StandardHours    |\n",
    "| Stock Option Level        | Indicator Variable: 0 - 3                    | HR     | StockOptionLevel |\n",
    "| Total Working Years       | Years Nominal Intergers                      | HR     | TotalWorkingYears |\n",
    "| Number of Training Times Last Year | Days                                | HR     | TrainingTimesLastYear |\n",
    "| Number of Years With Company | Years as Whole Number                     | HR     | YearsAtCompany   |\n",
    "| Number of Years Since Last Promotion |Years as Whole Number (Minimum = 0 | HR     | YearsSinceLastPromotion |\n",
    "| Number of Years with Current Manager | Years as Whole Number (Minimum = 0) | HR   | YearsWithCurrManager |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise II.A.2 \n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Determine the mean and standard deviation of the age of the employees.  Create and interpret a histogram of the age.\n",
    "\n",
    "[See Solution](#Solution-II.A.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for mean and standard deviation\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for histogram\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise II.A.3 \n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Standardize the age to have a zero mean and unit variance.  Determine the mean and standard deviation of the standarized age.  Create and interpret a histogram of the standardized age.\n",
    "\n",
    "[See Solution](#Solution-II.A.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here standardization\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for standardized histogram\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise II.A.4 \n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "The employees belong to three departments: *Human Resources*, *Research \\& Development*, and *Sales*.  The variable is named *Department*.  Determine the proportion of employees in each department, create a barplot of these proportions, and recode the departments with a one-hot ending.\n",
    "\n",
    "[See Solution](#Solution-II.A.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for proportion calculation\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for barplot\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for one-hot encoding\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '038' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson III: Supervised Learning Methods\n",
    "----------------------------------------------------------\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '040' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised vs. Unsupervised Learning\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '042' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '043' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide('044')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '045' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Model (GLM)\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '047' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '048' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test Split Data\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '050' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '051' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '052' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '053' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps for Predictive Modeling: Train/Test Split Data\n",
    "\n",
    "The data are split into two parts using *sklearn*.  Each part has a *X* variable array and a *y* vector (The upper and lower cases are conventional).  The *X* array is a Pandas DataFrame of the *X* variables.  The *y* vector is a Pandas Series.\n",
    "<br><br>\n",
    "Our Case Study data are panel data.  Let's collapse the time dimension and then split on the cross-sectional units.  This means we have to aggregate over the *CID* level.  We have to sum some varioables and find the mean of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Aggregate panel data to CID level for modeling\n",
    "##\n",
    "## Identify variables for modeling and aggregation\n",
    "##\n",
    "x = [ 'CID', 'Qtr', 'Region', 'loyaltyProgram', 'buyerRating', 'buyerSatisfaction', 'Usales', 'Pprice', \n",
    "     'Ddisc', 'Odisc', 'Cdisc', 'Pdisc']\n",
    "aggregations = { 'Usales':'sum', 'Pprice':'mean', 'Ddisc':'mean', 'Odisc':'mean', 'Cdisc':'mean',\n",
    "               'Pdisc':'mean'}\n",
    "##\n",
    "## Use grouby with agg function to aggregate\n",
    "##\n",
    "tmp = df[ x ]\n",
    "df_agg = tmp.groupby( [ 'CID', 'Qtr', 'Region', 'loyaltyProgram', 'buyerRating',\n",
    "                       'buyerSatisfaction' ] ).agg( aggregations )\n",
    "##\n",
    "## Rename columns and reset index\n",
    "##\n",
    "df_agg.rename( columns = { 'Usales':'totalUsales', 'Pprice':'meanPprice', 'Ddisc':'meanDdisc',\n",
    "                      'Odisc':'meanOdisc', 'Cdisc':'meanCdisc',\n",
    "                      'Pdisc':'meanPdisc'} , inplace = True )\n",
    "df_agg = df_agg.reset_index()\n",
    "df_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "This code block first specifies the data to subset and the types of aggregation to do on the numeric variables.  The aggregations are just *sum* and *mean*.  The *groupby* function does the aggregattion by groups specifed as *CID*, *Qtr*, etc.  The aggregated data are stored in te DataFrame *df_agg*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check the shape of df_agg\n",
    "##\n",
    "print( 'Rows: {}, \\tColumns: {}'.format( df_agg.shape[0], df_agg.shape[1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create the X and y data for splitting.  Use the aggregated data.\n",
    "##\n",
    "y = df_agg[ 'totalUsales' ]\n",
    "x = [ 'CID', 'Qtr', 'Region', 'loyaltyProgram', 'buyerRating', 'buyerSatisfaction', 'meanPprice',\n",
    "     'meanDdisc', 'meanOdisc', 'meanCdisc', 'meanPdisc' ]\n",
    "X = df_agg[ x ]\n",
    "##\n",
    "## Split the data: 1/4 and 3/4.  The default is 3/4 train.\n",
    "##\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.25,\n",
    "                                                    random_state = 42 )\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The dependent and independent variables need to be separated from the main DataFrame before the train/test split can be done.  The index from the main DataFrame is preserved.  The first three lines of code do this.  The *train_test_split* function randomly divides the data, keeping the indexes aligned.  The *random_state = 42* argument sets the random seed.  Four data sets are returned which are (in order): *X_train, X_test, y_train*, and *y_test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Display some data\n",
    "##\n",
    "print(\"Sample sizes: \\nX: {}, y: {}, 'Total: {}\\n\".format( X_train.shape[0], y_test.shape[0],\n",
    "                                                          X_train.shape[0] + y_test.shape[0]) )\n",
    "print( 'Training X Data: \\n{}'.format( X_train.head() ) )\n",
    "print( \"\\n\" )\n",
    "print( 'Training y Data: \\n{}'.format( y_train.head() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Note the indexes for the training and testing data sets.  These are the same as the main DataFrame, *df_agg*.  Notice that there are 870 records or *CID*s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the X and y training data for \n",
    "## model training.  Do an inner join on the indexes.\n",
    "##\n",
    "## Rename the y variable: totalUsales\n",
    "##\n",
    "yy = pd.DataFrame( { 'totalUsales':y_train } )\n",
    "ols_train = yy.merge( X_train, left_index = True, right_index = True )\n",
    "print( 'Training Data Set:\\n\\n{}'.format( ols_train.head() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The *X* and *Y* training data sets are merged on the indexes.  Recall that the index were preserved when the *y* and *X* data sets were created.  This is why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the X and y testing data sets for predicting.\n",
    "## Use an inner join on the indexes.\n",
    "##\n",
    "## Rename the y variable totalUsales.\n",
    "##\n",
    "yy = pd.DataFrame( { 'totalUsales':y_test } )\n",
    "ols_test = yy.merge( X_test, left_index = True, right_index = True )\n",
    "print( 'Testing Data Set:\\n\\n{}'.format( ols_test.head() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Add log Usales and log Pprice to the training data\n",
    "## The log is based on the Numpy function log1p\n",
    "## Note: log1p( x ) = log( 1 + x )\n",
    "##\n",
    "ols_train[ 'log_totalUsales' ] = np.log1p( ols_train.totalUsales )\n",
    "ols_train[ 'log_meanPprice' ] = np.log1p( ols_train.meanPprice )\n",
    "print( 'Training Data Set:\\n\\n{}'.format( ols_train.head() ) )\n",
    "print( \"\\n\" )\n",
    "print( 'Training Data Set Shape:\\n {}'.format( ols_train.shape ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "Logged terms are added because Data Visualization showed that logs induce normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Repeat for the testing data\n",
    "##\n",
    "ols_test[ 'log_totalUsales' ] = np.log1p( ols_test.totalUsales )\n",
    "ols_test[ 'log_meanPprice' ] = np.log1p( ols_test.meanPprice )\n",
    "print( 'Testing Data Set:\\n\\n{}'.format( ols_test.head() ) )\n",
    "print( \"\\n\" )\n",
    "print( 'Testng Data Set Shape:\\n {}'.format( ols_test.shape ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercises III.A\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise III.A.1 \n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "The employee DataFrame is cross-sectional.  Create training and testing data sets using the default 1/4, 3/4 split.  You will soon model the Percent Salary Hike (*PercentSalaryHike*) each employee last received so use this as the *y* variable.  The *X* variables are *Age*, *Department*, *TotalWorkingYears*, and *YearsAtCompany*.  Call the training set *train_hr* and the testing set *test_hr*, each with the prefix *ols*.\n",
    "\n",
    "[See Solution](#Solution-III.A.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise III.A.2 \n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Merge the training *X* and *y* data sets and then repeat for the two testing data sets.  Call the merged data set *ols_train_hr*.\n",
    "\n",
    "[See Solution](#Solution-III.A.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for training data\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for testing data\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Models: Identity Link\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '055' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '056' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## \n",
    "## OLS\n",
    "##\n",
    "## There are four steps for estimatng a model:\n",
    "##   1. define a formula (i.e., the specific model to estimate)\n",
    "##   2. instantiate the model (i.e., specify it)\n",
    "##   3. fit the model\n",
    "##   4. summarize the fitted model\n",
    "##\n",
    "## ===> Step 1: Define a formula\n",
    "##\n",
    "## The formula uses a “~” to separate the left-hand side from the right-hand side\n",
    "## of a model and a “+” to add columns to the right-hand side.  A “-” sign (not \n",
    "## used here) can be used to remove columns from the right-hand side (e.g.,\n",
    "## remove or omit the constant term which is always included by default). \n",
    "##\n",
    "formula = 'log_totalUsales ~ log_meanPprice + meanDdisc + meanOdisc + meanCdisc + meanPdisc + C( Region )'\n",
    "##\n",
    "## Since Region is categorical, you must create dummies for the regions.  You\n",
    "## do this using 'C( Region )' to indicate that Region is categorical.\n",
    "##\n",
    "## ===> Step 2: Instantiate the OLS model\n",
    "##\n",
    "mod = smf.ols( formula, data = ols_train )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model\n",
    "##      Recommendation: number your models\n",
    "##\n",
    "reg01 = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model\n",
    "##\n",
    "print( reg01.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The modeling follows four steps as shown above.  Regardless of the software you might use, these same four steps are followed.  Some software combines them, others require explicit statement.  This is what statsmodels requires.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The price elasticity is the coefficient for the logged price variable (i.e., log_Pprice): -2.6.  If price falls by 1\\%, unit sales rise by 2.6\\%.  This indicates that blinds are highly elastic.  This should be expected since furniture is a competitive business and blinds are very competitive.  Revenue will also change.  If price fall, revenue will increase.  The amount revenue will increase (in percentage terms) is given by $1 + elasticity$.  So for a 1\\% fall in price, revenue will rise 1.6\\% (= $1 + [-2.6]$). \n",
    "\n",
    "The discounts seem to have no effect, but this can be tested as we'll do below.  Also note that the $R^2$ is 0.28 which is very low.  \n",
    "\n",
    "The Jarque-Bera Test is a test for normality of the disturbance term.  It is a test of the \"goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution. $\\ldots$ The null hypothesis is a joint hypothesis of the skewness being zero and the excess kurtosis being zero.  $\\ldots$ If it is far from zero, it signals the data do not have a normal distribution.\"  So the Null Hypothesis is $H_O: Normality$.  (Source: <a href=\"https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test\" target=\"_parent\">see here</a>)  In this case, the Null is rejected.  The Omnibus Test is an alternative test of normality with the same Null.  It also indicates that the Null must be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises III.B\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise III.B.1\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Estimate an *OLS* model for *PercentSalaryHike* regressed on *Age*, *Department*, *TotalWorkingYears*, and *YearsAtCompany*.  Interpret the rsults.\n",
    "\n",
    "**Hint**: *Department* is categorical so you have to create dummies for it.\n",
    "\n",
    "[See Solution](#Solution-III.B.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Analyzing the Results </font>\n",
    "\n",
    "Quantities of interest can be extracted directly from the fitted model. Type *dir(results)* for a full list.\n",
    "\n",
    "Since the product manager wanted to know about a region effect, you should do an F-test of all the coefficients for the regions to determine if they are all zero, meaning that the dummies as a group do nothing.  This is a <u>joint</u> test of significance.  The test statistic is:\n",
    "\n",
    "$F_C = \\dfrac{\\left(SSR_U - SSR_R\\right)/(df_U - df_R)}{SSE_U/(n - p - 1)} = \\dfrac{\\left(SSE_R - SSE_U\\right)/(df_U - df_R)}{SSE_U/(n - p - 1)}$\n",
    "\n",
    "where \"U\" indicates the *unrestricted* or *full* model with the Region dummies and \"R\" indicates the *restricted* model without the Region dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Specify the joint (Null) hypothesis that the regions are the same;\n",
    "## i.e., there is no region effect.\n",
    "##\n",
    "hypothesis = ' ( C(Region)[T.Northeast] = 0, C(Region)[T.South] = 0, C(Region)[T.West] = 0 ) '\n",
    "##\n",
    "## Run and print an F-test \n",
    "##\n",
    "f_test = reg01.f_test( hypothesis )\n",
    "f_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "Notice that there are only three regions specified even though there are four: one is omitted as the base.  Also notice that the three hypotheses are specified as *C(Region)[T.XX] = 0* where *XX* is the region name.\n",
    "\n",
    "**_Output Interpretation_**\n",
    "\n",
    "The returned values for the F-test are, in order:\n",
    "\n",
    "1. The F-Statistic value\n",
    "2. The p-value for the F-Statistic\n",
    "3. The F-Statistic's denominator degrees-of-freedom\n",
    "4. The F-Statistic's numerator degrees-of-freedom\n",
    "\n",
    "**_Interpetation_**\n",
    "\n",
    "The Null Hypothesis is that there is no region effect.  The p-value is 0.0 so the Null Hypothesis is rejected: there is a Region effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Repeat the F-test for the discounts\n",
    "##\n",
    "hypothesis = ' ( meanDdisc = 0, meanOdisc = 0, meanCdisc = 0, meanPdisc = 0 ) '\n",
    "##\n",
    "## Run and print an F-test \n",
    "##\n",
    "f_test = reg01.f_test( hypothesis )\n",
    "f_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The hypothesis statement does not have the discount names as *C(Ddis)* etc. because they are quantitative variables, not categorical variables like *Region*. \n",
    "\n",
    "The Null Hypothesis is that there is no difference among the discounts; they all have zero effect on unit sales.  Notice that the p-value is 0.21.  So the Null Hypothesis that the discounts all have the same effect is not rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for multicollinearity -- a linear relationship among the variables.  You can use the corrlation matrix but this is only good for pair-wise relationships.  The *variance inflation factor* (*VIF*) is better.  A rule-of-thumb is that any $VIF > 10$ indicates a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Subset the design matrix to eliminate the first column of 1s\n",
    "## the iloc method says to find the location of columns based on \n",
    "## their integer locations (i.e., 0, 1, 2, etc.)\n",
    "## the term in brackets says to find all rows (the : ) and all \n",
    "## columns from the first to the end (1: )\n",
    "##\n",
    "## Create the correlation matrix\n",
    "##\n",
    "x = reg01.model.data.orig_exog.iloc[ :, 1: ] \n",
    "corr_matrix = x.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate VIFs\n",
    "## The VIFs are the diagonal elements of the inverted correlation\n",
    "## matrix of the independent variables.\n",
    "##\n",
    "## Subset the design matrix to eliminate the first column of 1s.\n",
    "## The iloc method says to find the location of columns based on their \n",
    "## integer locations (i.e., 0, 1, 2, etc.) the term in brackets says \n",
    "## to find all rows (the : ) and all columns from the first to the end (1: ).\n",
    "##\n",
    "## Create the correlation matrix\n",
    "##\n",
    "x = reg01.model.data.orig_exog.iloc[ :, 1: ]\n",
    "corr_matrix = x.corr()\n",
    "##\n",
    "## Invert the correlation matrix and extract the main diagonal\n",
    "##\n",
    "vif = np.diag( np.linalg.inv( corr_matrix ) ) \n",
    "##\n",
    "## Zip the variable names and the VIFs\n",
    "##\n",
    "indepvars = [ i for i in x.columns ]\n",
    "xzip = zip( indepvars, vif ) \n",
    "##\n",
    "## Display the zip matrix.  The lzip function was imported above.\n",
    "##\n",
    "lzip( xzip )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The *VIF*s are almost all below 10 so there is no problem.  $VIF > 10$ is a rule-of-thumb for indicating multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Predicting with the Model </font>\n",
    "\n",
    "Predict unit sales.  Recognize that sales are in (natural) log terms.  They need to be converted back to unit sales in \"normal\" terms by exponentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate predicted log of unit sales, the dependent variable.\n",
    "##\n",
    "## Note: the inverse of the log is needed; use np.expm1( x )\n",
    "## since log1p was used: np.expm1 = exp(x) - 1.\n",
    "##\n",
    "log_pred = reg01.predict( ols_test )\n",
    "y_pred = np.expm1( log_pred )\n",
    "##\n",
    "##\n",
    "## Combine into one temporary DataFrame for convenience\n",
    "##\n",
    "tmp = pd.DataFrame( { 'y_test':y_test, 'y_logPred':log_pred, 'y_pred':y_pred } )\n",
    "tmp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the sklearn metrics function *r2_score* to check the fit of actual vs. predicted values.  From the sklearn User Guide:\n",
    "\n",
    "\"*The r2_score function computes R², the coefficient of determination. It provides a measure of how well future samples are likely to be predicted by the model. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Display the r2 score.  But first drop any NaN data.\n",
    "##\n",
    "tmp.dropna( inplace = True )\n",
    "print( 'r2 Score:\\n {}'. format( round( r2_score( tmp.y_test, tmp.y_pred), 3 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The 0.191 is not very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict unit sales for different settings of the variables.  This is *scenario* or *what-if* analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Specify scenario values to use for prediction\n",
    "##\n",
    "## Create a dict\n",
    "##\n",
    "data = {\n",
    "         'meanPprice': [ 2.50 ],\n",
    "         'meanDdisc': [ 0.03 ],\n",
    "         'meanOdisc': [ 0.05 ],\n",
    "         'meanCdisc': [ 0.03 ],\n",
    "         'meanPdisc': [ 0.03 ],\n",
    "         'Region': [ 'West' ]\n",
    "        }\n",
    "##\n",
    "## Create a DataFrame using the dict\n",
    "##\n",
    "scenario = pd.DataFrame.from_dict( data )\n",
    "##\n",
    "## Insert a log price column after the Pprice variable\n",
    "##\n",
    "scenario.insert( loc = 1, column = 'log_meanPprice',\n",
    "                value = np.log1p( scenario.meanPprice ) )\n",
    "##\n",
    "## Display the settings and the predicted unit sales\n",
    "##\n",
    "print( 'Scenario settings:\\n{}'.format( scenario ) )\n",
    "##\n",
    "## Create a pediction\n",
    "##\n",
    "log_pred = reg01.predict( scenario )\n",
    "y_pred = np.expm1( log_pred )\n",
    "print( '\\nPredicted Unit Sales: \\n{}'.format( round( y_pred, 0 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression: Logit Link\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '059' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide('060' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '061' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '062' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Create your Data </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer satisfaction is part of the DataFrame.  Satisfaction is measured on a five-point scale: *1 = Not at All Satisfied*, *5 = Very Satisfied*.  \n",
    "\n",
    "First, look at the frquency count of satisfaction.  But, there is a problem: you cannot use the same data as before since satisfaction is by customer and the data used so far are by transaction.  The satisfaction rating is in the *df_agg* DataFrame.  So, there are three steps:\n",
    "\n",
    "1. Recode the scale values in the merged file so that 1 is the top-two values (called *top-two box* or *T2B*) and 0 is all other values.  The *T2B* is *Very Satisfied*.\n",
    "2. Split the data into training and testing datasets.\n",
    "3. Train a model with *T2B* satisfaction as a function of the pocket price, discounts, and Region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 1: Recode the scale values so that 1 is the top-two values \n",
    "## (called \"top-two box\" or \"T2B\") and 0 is all other values.  \n",
    "## The \"T2B\" is \"Very Satisfied\".\n",
    "##\n",
    "## Recode using Numpy's select function\n",
    "##\n",
    "## ===> Step 1.A: Define labels for the recoded values\n",
    "##\n",
    "lbl = [ 1, 0 ]   ## 1 = T2B satisfied; 0 = not satisfied\n",
    "##\n",
    "## ===> Step 1.B: Specify the conditions for the recoding\n",
    "##\n",
    "conditions = [\n",
    "    ( df_agg.buyerSatisfaction >= 4 ),\n",
    "    ( df_agg.buyerSatisfaction < 4 )\n",
    "]\n",
    "##\n",
    "## ===> Step 1.C: Do the recoding \n",
    "##\n",
    "df_agg[ 'sat_t2b' ] = np.select( conditions, lbl )\n",
    "##\n",
    "df_agg[ 'sat_t2b' ].value_counts( normalize = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model *T2B* satisfaction as a function of the pocket price and discounts.  First, create training and testing DataFrames as before but with *sat_t2b* as the *y* variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 2: Split the data.\n",
    "##\n",
    "## ===> Step 2.A: Create the X and y data for splitting\n",
    "##\n",
    "y = df_agg[ 'sat_t2b' ]\n",
    "x = [ 'meanPprice', 'meanDdisc', 'meanOdisc', 'meanCdisc', 'meanPdisc', 'Region', \n",
    "     'loyaltyProgram', 'buyerRating' ]\n",
    "X = df_agg[ x ]\n",
    "##\n",
    "## ===> Step 2.B: Split the data: 1/3 and 2/3.\n",
    "##\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.33, \n",
    "                                                    random_state = 42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Display some data\n",
    "##\n",
    "print( 'X Training Data: \\n{}'.format( X_train.head() ) ) \n",
    "print( \"\\n\" )\n",
    "print( 'Y Training Data: \\n{}'.format( y_train.head() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check counts\n",
    "##\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice that there are 582 training records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the two training sets for convenience\n",
    "##\n",
    "yy = pd.DataFrame( { 'sat_t2b':y_train } )\n",
    "logit_train = yy.merge( X_train, left_index = True, right_index = True )\n",
    "logit_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check counts\n",
    "##\n",
    "logit_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice that there are 582 training records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the two testing sets for convenience\n",
    "##\n",
    "yy = pd.DataFrame( { 'sat_t2b':y_test } )\n",
    "logit_test = yy.merge( X_test, left_index = True, right_index = True )\n",
    "logit_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check counts\n",
    "##\n",
    "logit_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice that there are 288 testing records.  The total of train + test is 582 + 288 = 870 which we had before for *df_agg*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color = black> Train a Model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 3: Train a logit model\n",
    "##\n",
    "## ===> Step 3A: Define a formula\n",
    "##\n",
    "formula = 'sat_t2b ~ meanPprice + meanDdisc + meanOdisc + meanCdisc + meanPdisc + C( Region )'\n",
    "##\n",
    "## ===> Step #b: Instantiate the logit model\n",
    "##\n",
    "mod = smf.logit( formula, data = logit_train )\n",
    "##\n",
    "## ===> Step 3C: Fit the instantiated model\n",
    "##\n",
    "logit01 = mod.fit()\n",
    "##\n",
    "## ===> Step 3D: Summarize the fitted model\n",
    "##\n",
    "print( logit01.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Predicting with the Model </font>\n",
    "\n",
    "The prediction process is the same as discussed for *Case I* above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises III.C\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise III.C.1\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "The employee DataFrame is cross-sectional.  Create training and testing data sets using the default 1/4, 3/4 split.  You will soon model the attrition (*Attrition*) so use this as the *y* variable.  The *X* variables are *Age*, *Department*, *TotalWorkingYears*, *YearsAtCompany*, and *YearsSinceLastPromotion*.  Call the training set *train_hr* and the testing set *test_hr*, each with the prefix *logit*.\n",
    "\n",
    "[See Solution](#Solution-III.C.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise III.C.2\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Merge the data sets.\n",
    "\n",
    "[See Solution](#Solution-III.C.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for merging the training data\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for merging the testing data\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise III.C.3\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Train a logit model.\n",
    "\n",
    "[See Solution](#Solution-III.C.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for training a logit model\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '065' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '066' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '068' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '069' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '070' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '071' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '072' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '073' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## NB can only handle numeric data, so Region, loyaltyProgram, and\n",
    "## buyerRating must be encoded using labelEncoder -- see package \n",
    "## load section above.\n",
    "##\n",
    "tmp = logit_train.copy()\n",
    "tmp.loc[ :, 'Region' ] = le.fit_transform( tmp.Region )\n",
    "tmp.loc[ :, 'loyaltyProgram' ] = le.fit_transform( tmp.loyaltyProgram )\n",
    "tmp.loc[ :, 'buyerRating' ] = le.fit_transform( tmp.buyerRating )\n",
    "nb_train = tmp.copy()\n",
    "nb_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## NB requires a separate X and y data set, so separate out\n",
    "## y = sat_t2b and X\n",
    "##\n",
    "y_train = nb_train[ 'sat_t2b' ]\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Separate X variables\n",
    "##\n",
    "x = [ 'meanPprice', 'meanDdisc', 'meanOdisc', 'meanCdisc', 'meanPdisc',\n",
    "       'Region', 'loyaltyProgram', 'buyerRating' ]\n",
    "X_train = nb_train[ x ]\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Repeat for testing data\n",
    "##\n",
    "tmp = logit_test.copy()\n",
    "tmp.loc[ :, 'Region' ] = le.fit_transform( tmp.Region )\n",
    "tmp.loc[ :, 'loyaltyProgram' ] = le.fit_transform( tmp.loyaltyProgram )\n",
    "tmp.loc[ :, 'buyerRating' ] = le.fit_transform( tmp.buyerRating )\n",
    "nb_test = tmp.copy()\n",
    "nb_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Repeat for testing y data\n",
    "##\n",
    "y_test = nb_test[ 'sat_t2b' ]\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Separate X variables\n",
    "##\n",
    "x = [ 'meanPprice', 'meanDdisc', 'meanOdisc', 'meanCdisc', 'meanPdisc',\n",
    "       'Region', 'loyaltyProgram', 'buyerRating' ]\n",
    "X_test = nb_test[ x ]\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 1: Instantiate a Gaussian Classifier\n",
    "##\n",
    "gnb = GaussianNB()\n",
    "##\n",
    "## === Step 2: Train the model using the training sets\n",
    "##\n",
    "gnb.fit(X_train, y_train)\n",
    "##\n",
    "## ===> Step 3: Predict the response for test dataset\n",
    "##\n",
    "y_pred = gnb.predict( X_test )\n",
    "##\n",
    "## ===> Step 4: Print predicted values\n",
    "##\n",
    "print( \"Predicted Value:\", y_pred )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Categorize as Over/Under/Equal on prediction\n",
    "##\n",
    "data = { 'test':y_test, 'predicted':y_pred }\n",
    "df_gnb = pd.DataFrame( data )\n",
    "##\n",
    "conditions = [\n",
    "    (df_gnb[ 'predicted'] > df_gnb[ 'test' ] ),\n",
    "    (df_gnb[ 'predicted'] < df_gnb[ 'test' ] )\n",
    "]\n",
    "choices = [ 'Over', 'Under' ]\n",
    "##\n",
    "df_gnb[ 'Over/Under' ] = np.select(conditions, choices, 'Equal' )\n",
    "##\n",
    "df_gnb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Prediction distribution\n",
    "##\n",
    "df_gnb[ 'Over/Under' ].value_counts( normalize = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create simple barchart\n",
    "##\n",
    "y = df_gnb[ 'Over/Under' ].value_counts( normalize = True )\n",
    "ax = sns.barplot( y = y, x = [ 'Equal', 'Over', 'Under' ] )\n",
    "ax.set( title = 'Naive Bayes Prediction Accuracy' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Model Accuracy, how often is the classifier correct?\n",
    "##\n",
    "print(\"Accuracy: {}\".format( metrics.accuracy_score( y_test, y_pred ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice that the accuracy measure is the same as the \"Equal\" group above.  They should be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '076' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '077' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '078' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '079' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '080' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the *X_train* and *y_train* data from the Naive Bayes.  Recall that *y_train* is the *sat_t2b\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 1: Create a SVM Classifier\n",
    "## Notice that SVC is used for the classifier\n",
    "##\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "##\n",
    "## ===> Step 2: Instantate the model using the training sets\n",
    "##\n",
    "clf.fit(X_train, y_train)\n",
    "##\n",
    "## ===> Step 3: Predict the response for test dataset\n",
    "##\n",
    "y_pred = clf.predict(X_test)\n",
    "##\n",
    "## ===> Step 4: Model Accuracy: how often is the classifier correct?\n",
    "##\n",
    "print(\"Accuracy: {}\".format( metrics.accuracy_score( y_test, y_pred ) ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '083' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '084' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '085' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '086' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '087' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '088' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Note: The Region variable was previously encoded as:\n",
    "##\n",
    "##          0: Midwest\n",
    "##          1: Northeast\n",
    "##          2: South\n",
    "##          3: West\n",
    "##\n",
    "## ===> Step 1: Instantiate the tree\n",
    "##\n",
    "dtree = tree.DecisionTreeClassifier( random_state = 0, max_depth = 3, \n",
    "                                    min_samples_leaf = 10 )\n",
    "##\n",
    "## ===> Step 2: Fit the tree\n",
    "##\n",
    "dtree.fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Both packages may have to be installed before they can be used.  \n",
    "## Use the operating system to do this.\n",
    "##\n",
    "##import os\n",
    "##!{sys.executable} -m pip install graphviz\n",
    "##!{sys.executable} -m pip install pydotplus\n",
    "\n",
    "## conda install -c anaconda graphviz python-graphviz pydotplus\n",
    "\n",
    "##\n",
    "## Tell Python where the graphviz package is load; then load it.\n",
    "##\n",
    "##os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "##\n",
    "## Load the following packages\n",
    "##\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Displaying a tree is a slight challenge!\n",
    "## There are four steps:\n",
    "##\n",
    "## ===> Step 1: Create a placeholder for all the plotting points.\n",
    "##\n",
    "dot_data = StringIO()\n",
    "##\n",
    "## ===> Step 2: Extract the feature names for labels models\n",
    "##\n",
    "feature_names = [ i for i in X_train.columns ]\n",
    "print( 'Feature names for tree:\\n{}'.format( feature_names ) )\n",
    "##\n",
    "## ===> Step 3: Export the plotting data to the placeholder\n",
    "##\n",
    "export_graphviz(dtree, out_file = dot_data,  \n",
    "                filled = True,\n",
    "                rounded = True,\n",
    "                special_characters = True,\n",
    "                class_names = [ 'B3B Sat', 'T2B Sat' ],  ## Order is 0 then 1\n",
    "                feature_names = feature_names,\n",
    "                proportion = True\n",
    "               )\n",
    "##\n",
    "## ===> Step 4: Create the display\n",
    "##\n",
    "graph = pydotplus.graph_from_dot_data( dot_data.getvalue() )  \n",
    "Image( graph.create_png() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '090' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson IV: Unsupervised Learning Methods\n",
    "----------------------------------------------------------\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '092' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types and Characteristics of Unsupervised Learning\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '094' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '096' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '098' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '099' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '100' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Subset the df_agg data\n",
    "##\n",
    "x = [ 'Qtr', 'Region', 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "df_hclusters = df_agg[ x ]\n",
    "df_hclusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Standardize the six numeric variables.\n",
    "##\n",
    "x = [ 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "df_hclusters.loc[ :, x ] = StandardScaler().fit_transform( df_hclusters[ x ] )\n",
    "df_hclusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Convert character labels to numerics\n",
    "##\n",
    "df_hclusters.loc[ :, 'Qtr' ] = le.fit_transform( df_hclusters[ 'Qtr' ] )\n",
    "df_hclusters.loc[ :, 'Region' ] = le.fit_transform( df_hclusters.Region )\n",
    "df_hclusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use Ward's minimum variance linkage method\n",
    "##\n",
    "ward = shc.linkage( df_hclusters, method = 'ward' )\n",
    "##\n",
    "## Plot a dendogram\n",
    "## WARNING:  this will take a minute\n",
    "##\n",
    "max_dist = 23\n",
    "##\n",
    "plt.figure(figsize=(10, 7))  \n",
    "plt.title( 'CID Clustering\\nHierarchical Clustering Dendrogram\\nWard\\'s Method' )\n",
    "plt.xlabel( 'Customer (CID)' )\n",
    "plt.ylabel( 'Distance' )\n",
    "dend = shc.dendrogram( ward )\n",
    "plt.axhline( y = max_dist, c = 'black', ls = '-', lw = 1.5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **_Interpretation_**\n",
    "\n",
    "A horizontal line is drawn at a distance of 25.  Clusters formed below this line is a group.  Also notice that there are five groups.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Identify the CIDs in each cluster\n",
    "## Consider any cluster grouping formed above 23\n",
    "##\n",
    "cluster_labels = fcluster( ward, max_dist, criterion = 'distance' )\n",
    "df_hclusters[ 'Cluster_Number' ] = cluster_labels\n",
    "df_hclusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Examine the cluster size distribution\n",
    "##\n",
    "df_hclusters.Cluster_Number.value_counts( normalize = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Bar chart of cluster sizes\n",
    "##\n",
    "y = df_hclusters[ 'Cluster_Number' ].value_counts( normalize = True )\n",
    "ax = sns.barplot( y = y, x = [ 'Cluster ' + str( c ) for c in y.index ] )\n",
    "ax.set( title = 'Hierarchical Clustering' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create a boxplot for each cluster for Order Discount\n",
    "##\n",
    "ax = sns.boxplot( x = 'Cluster_Number', y = 'meanOdisc', data = df_hclusters )\n",
    "ax.set( title = 'Order Discount\\nby Clusters\\nHierarchical Clustering', xlabel = 'Clusters',\n",
    "      ylabel = 'Order Discount' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercises IV.A\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise IV.A.1\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Create a hierarchical clustering using the following variables:\n",
    "\n",
    "1. Age\n",
    "2. MonthlyIncome\n",
    "3. PercentSalaryHike\n",
    "4. TotalWorkingYears\n",
    "5. YearsAtCompany\n",
    "\n",
    "HINT 1: The first step is to standardize these five variables.\n",
    "\n",
    "HINT 2: The variable *TotalWorkingYears* has missing values.\n",
    "\n",
    "[See Solution](#Solution-IV.A.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '103' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Set up data \n",
    "##\n",
    "x = [ 'Qtr', 'Region', 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "df_kclusters = df_agg[ x ]\n",
    "df_kclusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Subset the data for all numerics\n",
    "##\n",
    "x = [ 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "tmp = df_kclusters[ x ]\n",
    "##\n",
    "## Do KMeans\n",
    "##\n",
    "kmeans = KMeans(n_clusters = 3, random_state = 1234 ).fit( tmp )\n",
    "data = kmeans.cluster_centers_\n",
    "print( data )\n",
    "dataset = pd.DataFrame.from_records( data, columns = x )\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "## Add cluster labels to main cluster DataFrame\n",
    "##\n",
    "df_kclusters['Cluster_Number'] = kmeans.labels_   ## Notice the underscore\n",
    "df_kclusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Examine the cluster size distribution\n",
    "##\n",
    "df_kclusters.Cluster_Number.value_counts( normalize = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Bar chart of cluster sizes\n",
    "##\n",
    "y = df_kclusters[ 'Cluster_Number' ].value_counts( normalize = True )\n",
    "ax = sns.barplot( y = y, x = [ 'Cluster ' + str( c ) for c in y.index ] )\n",
    "ax.set( title = 'KMeans Clustering' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Boxplot of Order Discount\n",
    "##\n",
    "ax = sns.boxplot( y = 'meanOdisc', x = 'Cluster_Number', data = df_kclusters )\n",
    "ax.set( title = 'Mean Order Discount\\nby Clusters\\nKMeans Clustering', ylabel = 'Order Discount' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixture Models\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '106' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '107' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '108' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Mixture Models\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Set up data \n",
    "##\n",
    "x = [ 'Qtr', 'Region', 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "df_mclusters = df_agg[ x ]\n",
    "df_mclusters.head()\n",
    "##\n",
    "## sub\n",
    "x = [ 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "tmp = df_mclusters[ x ] \n",
    "##\n",
    "## ===> Step 1: Instantiate the model\n",
    "##\n",
    "gmm = GaussianMixture( n_components = 3 ) \n",
    "##  \n",
    "## ===> Step 2: Fit the model  \n",
    "##\n",
    "gmm.fit( tmp ) \n",
    "##  \n",
    "## ===> Step 3: Add cluster labels to main cluster DataFrame \n",
    "##\n",
    "cluster_number = gmm.predict( tmp ) \n",
    "df_mclusters['Cluster_Number'] = cluster_number\n",
    "df_mclusters.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Examine the cluster size distribution\n",
    "##\n",
    "df_mclusters.Cluster_Number.value_counts( normalize = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Bar chart of cluster sizes\n",
    "##\n",
    "y = df_mclusters[ 'Cluster_Number' ].value_counts( normalize = True )\n",
    "ax = sns.barplot( y = y, x = [ 'Cluster ' + str( c ) for c in y.index ] )\n",
    "ax.set( title = 'Gaussian Mixture Clustering' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Boxplot of Order Discount\n",
    "##\n",
    "ax = sns.boxplot( y = 'meanOdisc', x = 'Cluster_Number', data = df_mclusters )\n",
    "ax.set( title = 'Mean Order Discount\\nby Clusters\\nGaussian Mixture Clustering', ylabel = 'Order Discount' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '110' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson V: Model Evaluation\n",
    "----------------------------------------\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '112' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '113' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '114' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '115' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '116' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '117' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Analyze the Logit Modeling Results </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Make predictions\n",
    "## Use logit_test from logit section above\n",
    "##\n",
    "predictions = logit01.predict( logit_test )\n",
    "predictions_nominal = [ 0 if x < 0.5 else 1 for x in predictions]\n",
    "print( classification_report( y_test, predictions_nominal, digits = 3 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification, the count of **true negatives** ($tn$), **false negatives** ($fn$), **true positives** ($tp$), and **false positives** ($fp$) can be found from a *confusion matrix*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create a confusion matrix\n",
    "##\n",
    "x = confusion_matrix(y_test, predictions_nominal).ravel()\n",
    "##\n",
    "## zip the variable names and the confusion\n",
    "##\n",
    "lbl = [ 'tn', 'fp', 'fn', 'tp' ]\n",
    "##\n",
    "## display the zip matrix\n",
    "##\n",
    "from statsmodels.compat import lzip\n",
    "lzip( zip( lbl, x ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "There were 0 true negatives, 91 false positives, 0 false negatives, and 197 true positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create labels\n",
    "##\n",
    "lbl = ['B2B', 'T2B']\n",
    "##\n",
    "## Create the confusion matrix\n",
    "##\n",
    "cm = confusion_matrix( y_test, predictions_nominal )\n",
    "tmp = pd.DataFrame(data=cm, index = lbl, columns = lbl )\n",
    "print( 'Confusion Matrix: \\n{}'.format( tmp ) )\n",
    "##\n",
    "## Plot the confusion matrix\n",
    "##\n",
    "sns.set( font_scale = 1.4 )   #for label size\n",
    "##\n",
    "ax = sns.heatmap( cm/cm.sum(), annot = True, annot_kws = { \"size\": 16 } )  # font size\n",
    "ax.set( title = 'Confusion Matrix of the Classifier', xlabel = 'Predicted',\n",
    "       ylabel = 'True' )\n",
    "ax.set_xticklabels(lbl)\n",
    "ax.set_yticklabels(lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "68\\% of the cases were predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contact Information\n",
    "----------------------------\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '120' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solutions\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution II.A.1\n",
    "\n",
    "[Return to Exercise II.A.1](#Exercise-II.A.1)\n",
    "\n",
    "Import the employee data and print the first five records (i.e., the \"head\"). The data are in a CSV file named employees.csv. Call the imported data *df_hr* for consistency with later work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hr = pd.read_csv( r'../data/HR/employees.csv' )\n",
    "df_hr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution II.A.2\n",
    "\n",
    "[Return to Exercise II.A.2](#Exercise-II.A.2)\n",
    "\n",
    "Determine the mean and standard deviation of the age of the employees.  Create and interpret a histogram of the age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_hr.Age\n",
    "print( 'Mean of Age: {0:0.3f}\\nStandard Deviation of Age: {1:0.3f}'.format( x.mean(), x.std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Alternative\n",
    "##\n",
    "x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot( x )\n",
    "ax.set( title = 'Histogram of Age', xlabel = 'Employee Age', ylabel = 'Proportion' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution II.A.3\n",
    "\n",
    "[Return to Exercise II.A.3](#Exercise-II.A.3)\n",
    "\n",
    "Standardize the age to have a zero mean and unit variance.  Determine the mean and standard deviation of the standarized age.  Create and interpret a histogram of the standardized age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Standardize Age\n",
    "## Use preprocessor -- see package loading section\n",
    "##\n",
    "x = df_hr.Age\n",
    "x_standard = pp.scale( x )\n",
    "print( 'Mean of Standardized Age: {0:0.3f}\\nStandard Deviation of Standardized Age: {1:0.3f}'.\n",
    "      format( x_standard.mean(), x_standard.std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot histogram of standardized Age\n",
    "##\n",
    "ax = sns.distplot( x_standard )\n",
    "ax.set( title = 'Histogram of Standardized Age', xlabel = 'Age', ylabel = 'Proportion' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice that this distribution is centered at 0.0 as it should be since the mean was removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution II.A.4\n",
    "\n",
    "[Return to Exercise II.A.4](#Exercise-II.A.4)\n",
    "\n",
    "The employees belong to three departments: *Human Resources*, *Research \\& Development*, and *Sales*.  The variable is named *Department*.  Determine the proportion of employees in each department, create a barplot of these proportions, and recode the departments with a one-hot ending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use *value_counts* with the *normalize = True* argument\n",
    "##\n",
    "x = df_hr.Department\n",
    "x.value_counts( normalize = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Use the Seaborn *barplot* function\n",
    "##\n",
    "y = x.value_counts(normalize = True)\n",
    "ax = sns.barplot( y = y, x = y.index )\n",
    "ax.set( title = 'Department Distribution' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## one-hot encoding\n",
    "##\n",
    "enc = pp.OneHotEncoder()  ## Order is alphanumeric so Midwest is first\n",
    "tmp = df_hr[ ['Department' ] ]\n",
    "ohe = enc.fit_transform( tmp ).toarray()\n",
    "print( 'One-Hot Encoded Array:\\n{}'.format( ohe ) )\n",
    "## \n",
    "## Create a DataFrame for easier viewing\n",
    "##\n",
    "df_tmp = pd.DataFrame( ohe, columns = [\"HR\", \"R\\&D\", \"Sales\" ] )  ## Note the alphanumeric order\n",
    "df_ohe = pd.concat( [ df_hr.Department, df_tmp ], axis=1 )\n",
    "df_ohe.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution III.A.1\n",
    "\n",
    "[Return to Exercise III.A.1](#Exercise-III.A.1)\n",
    "\n",
    "The employee DataFrame is cross-sectional.  Create training and testing data sets using the default 1/4, 3/4 split.  You will soon model the Percent Salary Hike (*PercentSalaryHike*) each employee last received so use this as the *y* variable.  The *X* variables are *Age*, *Department*, *TotalWorkingYears*, and *YearsAtCompany*.  Call the training set *train_hr* and the testing set *test_hr*, each with the prefix *ols*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Create the X and y data for splitting.  Use the entire HR data.\n",
    "##\n",
    "y = df_hr[ 'PercentSalaryHike' ]\n",
    "x = [ 'Age', 'Department', 'TotalWorkingYears', 'YearsAtCompany' ]\n",
    "X = df_hr[ x ]\n",
    "##\n",
    "## Split the data: 1/4 and 3/4.  The default is 3/4 train.\n",
    "##\n",
    "X_train_hr, X_test_hr, y_train_hr, y_test_hr = train_test_split( X, y, test_size = 0.25,\n",
    "                                                    random_state = 42 )\n",
    "X_train_hr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution III.A.2\n",
    "\n",
    "[Return to Exercise III.A.2](#Exercise-III.A.2)\n",
    "\n",
    "Merge the training *X* and *y* data sets and then repeat for the two testing data sets.  Call the merged data set *ols_train_hr*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the X and y training data for \n",
    "## model training.  Do an inner join on the indexes.\n",
    "##\n",
    "## Rename the y variable: PercentSalaryHike\n",
    "##\n",
    "yy = pd.DataFrame( { 'PercentSalaryHike':y_train_hr } )\n",
    "ols_train_hr = yy.merge( X_train_hr, left_index = True, right_index = True )\n",
    "print( 'Training Data Set:\\n\\n{}'.format( ols_train_hr.head() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the X and y testing data for \n",
    "## model testing.  Do an inner join on the indexes.\n",
    "##\n",
    "## Rename the y variable: PercentSalaryHike\n",
    "##\n",
    "yy = pd.DataFrame( { 'PercentSalaryHike':y_test_hr } )\n",
    "ols_test_hr = yy.merge( X_test_hr, left_index = True, right_index = True )\n",
    "print( 'Testing Data Set:\\n\\n{}'.format( ols_test_hr.head() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution III.B.1\n",
    "\n",
    "[Return to Exercise III.B.1](#Exercise-III.B.1)\n",
    "\n",
    "Estimate an *OLS* model for *PercentSalaryHike* regressed on *Age*, *Department*, *TotalWorkingYears*, and *YearsAtCompany*.  Interpret the rsults.\n",
    "\n",
    "**Hint**: *Department* is categorical so you have to create dummies for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## OLS - Version 1\n",
    "##\n",
    "## ===> Step 1: Define a formula\n",
    "##\n",
    "formula = 'PercentSalaryHike ~ Age + C(Department) + TotalWorkingYears + YearsAtCompany'\n",
    "##\n",
    "## ===> Step 2: Instantiate the OLS model\n",
    "##\n",
    "mod = smf.ols( formula, data = ols_train_hr )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model\n",
    "##      Recommendation: number your models\n",
    "##\n",
    "reg01_hr = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model\n",
    "##\n",
    "print( reg01_hr.summary() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## OLS - Version 2: insignificant variabls dropped\n",
    "##\n",
    "## ===> Step 1: Define a formula\n",
    "##\n",
    "formula = 'PercentSalaryHike ~ Age + YearsAtCompany'\n",
    "##\n",
    "## ===> Step 2: Instantiate the OLS model\n",
    "##\n",
    "mod = smf.ols( formula, data = ols_train_hr )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model\n",
    "##      Recommendation: number your models\n",
    "##\n",
    "reg02_hr = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model\n",
    "##\n",
    "print( reg02_hr.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution III.C.1\n",
    "\n",
    "[Return to Exercise III.C.1](#Exercise-III.C.1)\n",
    "\n",
    "The employee DataFrame is cross-sectional.  Create training and testing data sets using the default 1/4, 3/4 split.  You will soon model the attrition (*Attrition*) so use this as the *y* variable.  The *X* variables are *Age*, *Department*, *TotalWorkingYears*, *YearsAtCompany*, and *YearsSinceLastPromotion*.  Call the training set *train_hr* and the testing set *test_hr*, each with the prefix *logit*.  Use 1/4 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create the X and y data for splitting\n",
    "##\n",
    "y = df_hr[ 'Attrition' ]\n",
    "x = [ 'Age', 'Department', 'TotalWorkingYears', 'YearsAtCompany', 'YearsSinceLastPromotion' ]\n",
    "X = df_hr[ x ]\n",
    "##\n",
    "## Split the data: 1/4 and 3/4.\n",
    "##\n",
    "X_train_hr, X_test_hr, y_train_hr, y_test_hr = train_test_split( X, y, test_size = 0.25, \n",
    "                                                    random_state = 42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution III.C.2\n",
    "\n",
    "[Return to Exercise III.C.2](#Exercise-III.C.2)\n",
    "\n",
    "Merge the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the two training sets for convenience\n",
    "##\n",
    "yy = pd.DataFrame( { 'Attrition':y_train_hr } )\n",
    "logit_train_hr = yy.merge( X_train_hr, left_index = True, right_index = True )\n",
    "logit_train_hr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the two testing sets for convenience\n",
    "##\n",
    "yy = pd.DataFrame( { 'Attrition':y_test_hr } )\n",
    "logit_test_hr = yy.merge( X_test_hr, left_index = True, right_index = True )\n",
    "logit_test_hr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution III.C.3\n",
    "\n",
    "[Return to Exercise III.C.3](#Exercise-III.C.3)\n",
    "\n",
    "Recode the Attrition variable in both data sets so that $Yes = 1$ and $No = 0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Recode using Numpy's select function\n",
    "##\n",
    "## Define labels for the recoded values\n",
    "##\n",
    "lbl = [ 0, 1 ]   ## 1 = Yes; 0 = No\n",
    "##\n",
    "## Specify the conditions for the recoding\n",
    "##\n",
    "conditions = [\n",
    "    ( logit_train_hr.Attrition == 'No' ),\n",
    "    ( logit_train_hr.Attrition == 'Yes' )\n",
    "]\n",
    "##\n",
    "## Do the recoding \n",
    "##\n",
    "logit_train_hr[ 'att' ] = np.select( conditions, lbl )\n",
    "##\n",
    "logit_train_hr[ 'att' ].value_counts( normalize = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## It is a good idea to check some records\n",
    "## to make sure the recoding is correct\n",
    "##\n",
    "logit_train_hr[ [ 'Attrition', 'att' ] ].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Recode using Numpy's select function\n",
    "##\n",
    "## Define labels for the recoded values\n",
    "##\n",
    "lbl = [ 0, 1 ]   ## 1 = Yes; 0 = No\n",
    "##\n",
    "## Specify the conditions for the recoding\n",
    "##\n",
    "conditions = [\n",
    "    ( logit_test_hr.Attrition == 'No' ),\n",
    "    ( logit_test_hr.Attrition == 'Yes' )\n",
    "]\n",
    "##\n",
    "## Do the recoding \n",
    "##\n",
    "logit_test_hr[ 'att' ] = np.select( conditions, lbl )\n",
    "##\n",
    "logit_test_hr[ 'att' ].value_counts( normalize = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution III.C.4\n",
    "\n",
    "[Return to Exercise III.C.4](#Exercise-III.C.4)\n",
    "\n",
    "Train a logit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Define a formula\n",
    "##\n",
    "formula = 'att ~ Age + C(Department) + TotalWorkingYears + YearsAtCompany + YearsSinceLastPromotion'\n",
    "##\n",
    "## Instantiate the logit model\n",
    "##\n",
    "mod = smf.logit( formula, data = logit_train_hr )\n",
    "##\n",
    "## Fit the instantiated model\n",
    "##\n",
    "logit01_hr = mod.fit()\n",
    "##\n",
    "## ===> Step 3D: Summarize the fitted model\n",
    "##\n",
    "print( logit01_hr.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution IV.A.1\n",
    "\n",
    "[Return to Exercise IV.A.1](#Exercise-IV.A.1)\n",
    "\n",
    "Create a hierarchical clustering using the following variables:\n",
    "\n",
    "1. Age\n",
    "2. MonthlyIncome\n",
    "3. PercentSalaryHike\n",
    "4. TotalWorkingYears\n",
    "5. YearsAtCompany\n",
    "\n",
    "HINT 1: The first step is to standardize these five variables.\n",
    "HINT 2: The variable *TotalWorkingYears* has missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Make a list of the five numeric variables.\n",
    "##\n",
    "x = [ 'Age', 'MonthlyIncome', 'PercentSalaryHike', 'TotalWorkingYears', 'YearsAtCompany' ]\n",
    "##\n",
    "## Copy the five variables\n",
    "##\n",
    "df_hclusters_hr = df_hr[ x ].copy()\n",
    "##\n",
    "## Drop missing values\n",
    "##\n",
    "df_hclusters_hr.dropna( inplace = True )\n",
    "##\n",
    "## Standardize\n",
    "##\n",
    "df_hclusters_hr.loc[ :, x ] = StandardScaler().fit_transform( df_hclusters_hr[ x ] )\n",
    "df_hclusters_hr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use Ward's minimum variance linkage method\n",
    "##\n",
    "ward_hr = shc.linkage( df_hclusters_hr, method = 'ward' )\n",
    "##\n",
    "## Plot a dendogram\n",
    "## WARNING:  this will take a minute\n",
    "##\n",
    "max_dist_hr = 60\n",
    "##\n",
    "plt.figure(figsize=(10, 7))  \n",
    "plt.title( 'Employee Clustering\\nHierarchical Clustering Dendrogram\\nWard\\'s Method' )\n",
    "plt.xlabel( 'Employees' )\n",
    "plt.ylabel( 'Distance' )\n",
    "dend_hr= shc.dendrogram( ward_hr )\n",
    "plt.axhline( y = max_dist_hr, c = 'black', ls = '-', lw = 1.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Identify the employees in each cluster\n",
    "## Consider any cluster grouping formed above 60\n",
    "##\n",
    "cluster_labels_hr = fcluster( ward_hr, max_dist_hr, criterion = 'distance' )\n",
    "df_hclusters_hr[ 'Cluster_Number' ] = cluster_labels_hr\n",
    "df_hclusters_hr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Bar chart of cluster sizes\n",
    "##\n",
    "y = df_hclusters_hr[ 'Cluster_Number' ].value_counts( normalize = True )\n",
    "ax = sns.barplot( y = y, x = [ 'Cluster ' + str( c ) for c in y.index ] )\n",
    "ax.set( title = 'Employee Hierarchical Clustering' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
