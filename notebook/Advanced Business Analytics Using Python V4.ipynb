{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = red>Machine Learning for Business Analytics:<br>A Deep Dive into Data with Python</font>\n",
    "=======\n",
    "<br>\n",
    "    <center><img src=\"http://dataanalyticscorp.com/wp-content/uploads/2018/03/logo.png\"></center>\n",
    "<br>\n",
    "Taught by: \n",
    "\n",
    "* Walter R. Paczkowski, Ph.D. \n",
    "\n",
    "    * My Affliations: [Data Analytics Corp.](http://www.dataanalyticscorp.com/) and [Rutgers University](https://economics.rutgers.edu/people/teaching-personnel)\n",
    "    * [Email Me With Questions](mailto:walt@dataanalyticscorp.com)\n",
    "    * [Learn About Me](http://www.dataanalyticscorp.com/)\n",
    "    * [See My LinkedIn Profile](https://www.linkedin.com/in/walter-paczkowski-a17a1511/)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## slide code\n",
    "##\n",
    "from IPython.display import Image\n",
    "def slide(what):\n",
    "    display( Image( \"../Slides/ABA_Page_\" + what + \".png\", width = 50, height = 50, retina = True ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents\n",
    "-------------\n",
    "\n",
    "1. [**_Helpful Background_**](#Helpful-Background)\n",
    "    1. [About this Notebook](#About-this-Notebook)\n",
    "    2. [Helpful Online Tutorials](#Helpful-Online-Tutorials)\n",
    "    3. [Helpful Must-Read Book](#Helpful-Must-Read-Book)\n",
    "2. [**_Lesson 0: Preliminary Stuff_**](#Lesson-0:-Preliminary-Stuff)\n",
    "    1. [Load Python Packages](#Load-Python-Packages)\n",
    "    2. [Set Data Path](#Set-Data-Path)\n",
    "3. [**_Lesson I: Introduction_**](#Lesson-I:-Introduction)\n",
    "    1. [Case Study Data Dictionary](#Case-Study-Data-Dictionary)\n",
    "    2. [Load Case Study Data](#Load-Case-Study-Data)\n",
    "4. [**_Lesson II: Data Preprocessing_**](#Lesson-II:-Data-Preprocessing)\n",
    "    1. [Preprocessing Step I: Transformation](#Preprocessing-Step-I:-Transformation)\n",
    "    - [Preprocessing Step II: Encoding](#Preprocessing-Step-II:-Encoding)\n",
    "    - [Preprocessing Step III: Dimension Reduction](#Preprocessing-Step-III:-Dimension-Reduction)\n",
    "    2. [Exercises II.A](#Exercises-II.A)\n",
    "        1. [Exercise II.A.1](#Exercise-II.A.1)\n",
    "        - [Exercise II.A.2](#Exercise-II.A.2)\n",
    "        - [Exercise II.A.3](#Exercise-II.A.3)\n",
    "        - [Exercise II.A.4](#Exercise-II.A.4)\n",
    "5. [**_Lesson III: Supervised Learning Methods_**](#Lesson-III:-Supervised-Learning-Methods)\n",
    "    1. [Supervised vs. Unsupervised Learning](#Supervised-vs.-Unsupervised-Learning)\n",
    "    - [Generalized Linear Model (GLM)](#Generalized-Linear-Model-(GLM))\n",
    "        1. [Train/Test Split Data](#Train/Test-Split-Data)\n",
    "        - [Exercsies III.A](#Exercises-III.A)\n",
    "            1. [Exercise III.A.1](#Exercise-III.A.1)\n",
    "            - [Exercise III.A.2](#Exercise-III.A.2)\n",
    "        - [Linear Models: Identity Link](#Linear-Models:-Identity-Link)\n",
    "        - [Exercsies III.B](#Exercises-III.B)\n",
    "            1. [Exercise III.B.1](#Exercise-III.B.1)\n",
    "        - [Logistic Regression: Logit Link](#Logistic-Regression:-Logit-Link)\n",
    "        - [Exercsies III.C](#Exercises-III.C)\n",
    "            1. [Exercise III.C.1](#Exercise-III.C.1)\n",
    "            - [Exercise III.C.2](#Exercise-III.C.2)\n",
    "            - [Exercise III.C.3](#Exercise-III.C.3)\n",
    "    - [Classification](#Classification)\n",
    "        1. [Naive Bayes](#Naive-Bayes)\n",
    "        - [Support Vector Machines](#Support-Vector-Machines)\n",
    "        - [Decision Trees](#Decision-Trees)\n",
    "6. [**_Lesson IV: Unsupervised Learning Methods_**](#Lesson-IV:-Unsupervised-Learning-Methods)\n",
    "    1. [Types and Characteristics of Unsupervised Learning](#Types-and-Characteristics-of-Unsupervised-Learning)\n",
    "    - [Clustering](#Clustering)\n",
    "        1. [Hierarchical](#Hierarchical)\n",
    "        - [Exercsies IV.A](#Exercises-IV.A)\n",
    "            1. [Exercise IV.A.1](#Exercise-IV.A.1)        \n",
    "        - [K-Means](#K-Means)\n",
    "        - [Mixture Models](#Mixture-Models)\n",
    "        - [Gaussian Mixture Models](#Gaussian-Mixture-Models)\n",
    "7. [**_Lesson V: Model Evaluation_**](#Lesson-V:-Model-Evaluation)\n",
    "8. [**_Contact Information_**](#Contact-Information)\n",
    "9. [**_Exercise Solutions_**](#Exercise-Solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful Background\n",
    "----------------------------\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About this Notebook\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "This notebook accompanies the PDF presentation **_Machine Learning for Business Analytics: A Deep Dive into Data with Python_** by Walter R. Paczkowski, Ph.D. (2019).  There is more content and commentary in this notebook than in the presentation deck.  Nonetheless, the two complement each other and so should be studied together.  Every effort has been made to use the same key slide titles in the presentation deck and this notebook to help your learning.  For your convenience, most of the presentation deck slides have been incorporated into this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Online Tutorials\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "* <a href=\"http://docs.python.org/2/tutorial/\" target=\"_parent\">Python Tutorial</a>\n",
    "\n",
    "* <a href=\"https://pandas.pydata.org/pandas-docs/stable/getting_started/tutorials.html\" target=\"_parent\">Pandas Tutorial</a>\n",
    "\n",
    "* <a href=\"https://seaborn.pydata.org/tutorial.html\" target=\"_parent\">Seaborn Tutorial</a>\n",
    "\n",
    "* <a href=\"https://www.statsmodels.org/stable/index.html\" target=\"_parent\">Statsmodels Tutorial</a>\n",
    "\n",
    "* <a href=\"https://scikit-learn.org/stable/tutorial/index.html\" target=\"_parent\">scikit-learn Tutorials</a>\n",
    "\n",
    "\n",
    "### Helpful Must-Read Book\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "* <a href=\"https://www.amazon.com/gp/product/1491957662/ref=as_li_tl?ie=UTF8&tag=quantpytho-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1491957662&linkId=8c3bf87b221dbcd8f541f0db20d4da83\" target=\"_parent\">Main Pandas go-to book: </a> *Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython* (2nd Edition) by Wes McKinney.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson 0: Preliminary Stuff\n",
    "--------------------------------------\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Python Packages\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "You have to load a Python package before you can use it.  Loading is done using an *import* command.  An alias is assigned when you import the package.  I recommend loading all the packages at once at the beginning of your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Data Management <===\n",
    "##\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "##\n",
    "pd.set_option( 'display.max_columns', 8 )\n",
    "##\n",
    "## ===> Visualization <===\n",
    "##\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "##\n",
    "## ===> Analytical <===\n",
    "##\n",
    "## sklearn packages:\n",
    "##   1. preprocessing\n",
    "##   2. principal components\n",
    "##   3. MinMaxScaler\n",
    "##   4. standardScaler\n",
    "##   5. label encoder\n",
    "##\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "##\n",
    "## Train_test_split\n",
    "##\n",
    "from sklearn.model_selection import train_test_split\n",
    "##\n",
    "## Modeling\n",
    "##   Notice the new import command for\n",
    "##   the formula API and the summary option\n",
    "##\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf \n",
    "##\n",
    "## Confusion matrix functions\n",
    "##\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "##\n",
    "## r2_score function from the sklearn metrics package\n",
    "##\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "##\n",
    "## Decision tree classifier\n",
    "##\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_graphviz\n",
    "##\n",
    "## Miscellaneous packages for Decision Trees\n",
    "##\n",
    "## Grahpviz and pydotplus may have to be installed before they can be used.  \n",
    "## Use the operating system to do this.\n",
    "##\n",
    "##import os\n",
    "##!{sys.executable} -m pip install graphviz\n",
    "##!{sys.executable} -m pip install pydotplus\n",
    "##\n",
    "## conda install -c anaconda graphviz python-graphviz pydotplus\n",
    "##\n",
    "## Tell Python where the graphviz package is loaded; then load it.\n",
    "##\n",
    "##os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "##\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "import graphviz\n",
    "##\n",
    "## Clustering \n",
    "##\n",
    "##   ===> Hierarchical\n",
    "##\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "##\n",
    "##    ===> KMeans Clustering\n",
    "##\n",
    "from sklearn.cluster import KMeans\n",
    "##\n",
    "##    ===> Gaussian Mixture Model\n",
    "##\n",
    "from sklearn.mixture import GaussianMixture \n",
    "##\n",
    "## Image for displaying slides\n",
    "##\n",
    "from IPython.display import Image\n",
    "##\n",
    "## Gaussian Naive Bayes\n",
    "##\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "##\n",
    "## SVM\n",
    "##\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Data Path\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "It is best practice to define paths in one location.  This makes error finding and changes easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Set data path\n",
    "##\n",
    "path = '../Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson I: Introduction\n",
    "-------------------------------\n",
    "\n",
    "[Back to Contents](#Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '002' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '004' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '006' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '008' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '010' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '011' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study Data Dictionary\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "| Variable                  | Values                                 | Source       | Mnemonic     |\n",
    "|---------------------------|----------------------------------------|--------------|--------------|\n",
    "| Order Number              | Nominal Integer                        | Order Sys    | Onum         |\n",
    "| Customer ID               | Nominal                                | Customer Sys | CID          | \n",
    "| Transaction Date          | MM/DD/YYYY                             | Order Sys    | Tdate        | \n",
    "| Product Line ID           | Five rooms of house                    | Product Sys  | Pline        |\n",
    "| Product Class ID          | Item in line                           | Product Sys  | Pclass       |\n",
    "| Units Sold                | Number of units per order              | Order Sys    | Usales       |\n",
    "| Product Returned?         | Yes/No                                 | Order Sys    | Return       |\n",
    "| Amount Returned           | Number of units                        | Order Sys    | returnAmount |\n",
    "| Material Cost/Unit        | \\$US cost of material                  | Product Sys  | Mcost        |\n",
    "| List Price                | \\$US list                              | Price Sys    | Lprice       |\n",
    "| Dealer Discount           | \\% discount to dealer (decimal)        | Sales Sys    | Ddisc        |\n",
    "| Competitive Discount      | \\% discount for competition (decimal)  | Sales Sys    | Cdisc        |\n",
    "| Order Size Discount       | \\% discount for size (decimal)         | Sales Sys    | Odisc        |\n",
    "| Customer Pickup Allowance | \\% discount for pickup (decimal)       | Sales Sys    | Pdisc        |\n",
    "| Total Discount            | \\% discount                            | Calculated: Sum of discounts | Tdisc         |\n",
    "| Pocket Price              | \\$US                                   | Calculated: LPrice $\\times$ (1 - TDisc) | Pprice  | \n",
    "| Log of Unit Sales         | Log sales                              | Calculated: log(Usales)  | log_Usales  |\n",
    "| Log of Pocket Price       | \\$US                                   | Calculated: log(Pprice)  | log_Pprice  |\n",
    "| Revenue                   | \\$US                                   | Calculated: Usales $\\times$ Pprice | Rev          |\n",
    "| Contribution              | \\$US                                   | Calculated: Rev - Mcost | Con  |\n",
    "| Contribution Margin       | \\%                                     | Calculated: Con/Rev | CM |\n",
    "| Net Revenue               | \\$US                                   | Calculated: (Usales - returnAmount) $\\times$  Pprice  | netRev  |\n",
    "| Lost Revenue         |  \\$US   | Calculated: Rev - netRev  | lostRev  | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Case Study Data\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Import the data.  The parse_dates argument says to \n",
    "## treat Tdate as a date object.\n",
    "##\n",
    "file = 'orders.csv'\n",
    "df_orders = pd.read_csv( path + file, parse_dates = [ 'Tdate' ] )\n",
    "##\n",
    "## Initial Calculations on first DataFrame\n",
    "##\n",
    "x = [ 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc' ]\n",
    "df_orders[ 'Tdisc' ] = df_orders[ x ].sum( axis = 1 )  ## Total discounts\n",
    "##\n",
    "df_orders[ 'Pprice' ] = df_orders.Lprice*( 1 - df_orders.Tdisc )  ## Pocket prices\n",
    "##\n",
    "df_orders[ 'Rev' ] = df_orders.Usales * df_orders.Pprice  ## Gross revenue\n",
    "df_orders[ 'netRev' ] = ( df_orders.Usales - df_orders.returnAmount )*df_orders.Pprice  ## Net revenue\n",
    "df_orders[ 'lostRev' ] = df_orders.Rev - df_orders.netRev  ## Lost revenue due to returns\n",
    "##\n",
    "df_orders[ 'Con' ] = df_orders.Rev - df_orders.Mcost  ## Contribution (i.e., profit)\n",
    "df_orders[ 'CM' ] = df_orders.Con/df_orders.Rev  ## Contribution margin\n",
    "##\n",
    "## Import a second DataFrame on the customers\n",
    "##\n",
    "file = 'customers.csv'\n",
    "df_cust = pd.read_csv( path + file )\n",
    "##\n",
    "## Do an inner join using CID as the link\n",
    "##\n",
    "df = pd.merge( df_orders, df_cust, on = 'CID' )\n",
    "##\n",
    "## Display shape and head of the final DataFrame\n",
    "##\n",
    "print( 'Number of rows: {rows}\\nNumber of columns: {cols}'.format( \n",
    "        rows = df.shape[ 0 ], cols = df.shape[ 1 ] ) )\n",
    "df.head().style.set_caption( 'Base DataFrame' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "\n",
    "The *set_caption* style method sets a caption.  Other style functions will be shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future reference, count the number of unique *CID*s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## How many unique CIDs are available?\n",
    "##\n",
    "x = df.CID.nunique()\n",
    "print( 'Number of unique CIDs: {}'.format( x ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson II: Data Preprocessing\n",
    "------------------------------------------\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '013' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '014' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '015' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Step I: Transformation\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slide( '017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot unit sales histogram\n",
    "##\n",
    "x = df.Usales\n",
    "ax = sns.distplot( x )\n",
    "ax.set( title = 'Histogram of Unit Sales', xlabel = 'Unit Sales', ylabel = 'Proportion' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The Seaborn function for a histogram is *displot*.  It takes an argument of the variable to plot.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "Note the right skewness and centering of the data around 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Print mean and standard deviation of original unit sales\n",
    "##\n",
    "print( 'Mean of Unit Sales: {0:0.1f}\\nStandard Deviation of Unit Sales: {1:0.1f}'.\\\n",
    "      format( x.mean(), x.std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Take log of sales to normalize the data\n",
    "##\n",
    "log_x = np.log( df.Usales )\n",
    "ax = sns.distplot( log_x )\n",
    "ax.set( title = 'Histogram of Unit Sales\\nLog Scale', xlabel = 'Log of Unit Sales',\n",
    "       ylabel = 'Proportion' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "\n",
    "The natural log tends to normalize data.  The log function is in the Numpy package and is simply *log*.  Another log function will be used later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The distribution of log unit sales is more normal.  This will be important for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Standardize unit sales\n",
    "## Use preprocessor package -- see package loading section\n",
    "##\n",
    "x = df.Usales\n",
    "x_standard = pp.scale( x )\n",
    "print( 'Mean of Standardized Unit Sales: {0:0.1f}\\nStandard Deviation of Standardized Unit Sales: {1:0.1f}'.\n",
    "      format( x_standard.mean(), x_standard.std() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The preprocessor alias is *pp*.  The function *scale* does zero mean (centering at zero) unit variance scaling by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot histogram of standardized unit sales\n",
    "##\n",
    "ax = sns.distplot( x_standard )\n",
    "ax.set( title = 'Histogram of Standardized Unit Sales', xlabel = 'Unit Sales', ylabel = 'Proportion' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice that the x-axis scale differs from the previous histogram but that the shape of the distribution is unchanged.  As a linear transformation, the distribution is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Transform to a [0, 1] range using the MinMaxScaler\n",
    "##\n",
    "tmp = df[ ['Usales' ] ].copy()\n",
    "scaler = MinMaxScaler()\n",
    "tmp[ 'Usales_minMax_scaled' ] = scaler.fit_transform( tmp )\n",
    "tmp.head().style.format( '{:,.4f}' ).set_caption( 'MinMax Scaling' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The *copy()* function makes a copy of the DataFrame (keeping only the one variable in this case) so that the integrity of the original DataFrame is preserved.  This is best practice.  The *fit_transform* function first fits (i.e., determines) the min and max values then transforms the data by the min-max formula.  Both steps of fit followed by transform are done by one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check descriptive statistics\n",
    "##\n",
    "tmp.describe().T.style.format( '{:,.4f}' ).set_caption( 'Descriptive Statistics' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "\n",
    "The *T* is the transpose function.  The *style* method for the DataFrame sets the format to four decimal places.    Other styles will be used below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "Notice that for *Usales_minMax_scaled*, the min is 0 and the max is 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Step II: Encoding\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '027' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '028' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '029' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '030' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '031' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '032' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## One-hot encoding\n",
    "##\n",
    "enc = pp.OneHotEncoder()  \n",
    "tmp = df[ ['Region' ] ]\n",
    "ohe = enc.fit_transform( tmp ).toarray()\n",
    "## \n",
    "## Create a DataFrame for easier viewing\n",
    "##\n",
    "col_names = [\"Region_\" + str( int( i ) ) for i in range( ohe.shape[ 1 ] ) ]\n",
    "df_ohe = pd.DataFrame(ohe, columns = col_names )\n",
    "df_ohe = pd.concat( [ df.Region, df_ohe ], axis = 1 )\n",
    "##\n",
    "df_ohe.head().style.set_caption( 'One-Hot Encoded Array in a DataFrame:' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The order for the one-hot encoder is alphanumeric so Midwest is first and is thus dropped to avoid perfect multicollinearity in linear modeling as shown below.  See [here](https://en.wikipedia.org/wiki/Multicollinearity) for multicollinearity.  The *toarray()* function puts the results into an array as shown.  A list comprehension creates the DataFrame column labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Step III: Dimension Reduction\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '035' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '036' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Subset features for dimension reduction\n",
    "##\n",
    "x = [ 'Odisc', 'Cdisc', 'Ddisc', 'Pdisc' ]\n",
    "x_standard = df[ x ].copy()\n",
    "##\n",
    "## Drop NaN since some discounts have missing values\n",
    "## Do inplace\n",
    "##\n",
    "x_standard.dropna( inplace = True )\n",
    "##\n",
    "## View the head\n",
    "##\n",
    "print( 'Unstandardized discounts:\\n{}'.format( x_standard.head() ) )\n",
    "##\n",
    "## Standardize the discounts\n",
    "##\n",
    "x_standard = StandardScaler().fit_transform( x_standard )\n",
    "##\n",
    "## Put standardized discounts in a DataFrame and view head\n",
    "##\n",
    "col_names = [x + '_std' for x in x ] \n",
    "df_x_standard = pd.DataFrame( x_standard, columns = col_names )\n",
    "print( '\\nStandardized discounts\\n{}'.format( df_x_standard.head() ) )\n",
    "##\n",
    "## Check descriptive statistics of standardized data\n",
    "##\n",
    "print( '\\nDescriptive statistics:\\n{}'.format( df_x_standard.describe().T.round( 1 ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Find principal components\n",
    "##\n",
    "## Specify number of components to extract: one for each discount\n",
    "##\n",
    "n_components = x_standard.shape[ 1 ]\n",
    "##\n",
    "## Do PCA\n",
    "##\n",
    "pca = PCA( n_components = n_components )\n",
    "principalComponents = pca.fit_transform( x_standard )\n",
    "##\n",
    "## Extract all four components and put in DataFrame\n",
    "## First define column names\n",
    "##\n",
    "col_names = [ 'pc_' + str( x ) for x in range( 1, n_components + 1 ) ]\n",
    "df_pca = pd.DataFrame( data = principalComponents,\n",
    "             columns = col_names )\n",
    "df_pca.head().style.format( '{:,.4f}' ).set_caption( 'Principal components' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check correlation matrix.  It should be the identity matrix.\n",
    "##\n",
    "df_pca.corr().style.format( '{:,.2f}' ).set_caption( 'Correlation Matrix' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "Notice that the principal components are independent of each other (zero off-diagonal correlations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Explained variance\n",
    "##\n",
    "x = pca.explained_variance_\n",
    "##\n",
    "df_explained_var = pd.DataFrame( x, columns = [ 'Variance' ], index = col_names )\n",
    "##\n",
    "df_explained_var.style.format( '{:,.4f}' ).set_caption( 'Explained variance' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "These are the variance components accounted for by each principal component, in descending order.  The sum of these variance components is the total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Explained variance: proportion\n",
    "##\n",
    "x = pca.explained_variance_ratio_\n",
    "##\n",
    "df_explained_var_prop = pd.DataFrame( x, columns = [ 'Variance_proportion' ], index = col_names )\n",
    "##\n",
    "df_explained_var_prop.style.format( '{:,.4f}' ).set_caption( 'Explained variance proportions' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "These are the proportion of total data variance accounted for by each principal component, in descending order.  They sum to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Singular values\n",
    "##\n",
    "x = pca.singular_values_\n",
    "##\n",
    "df_singular_values = pd.DataFrame( x, columns = [ 'Singular_Values' ], index = col_names )\n",
    "##\n",
    "df_singular_values.style.format( '{:,.2f}' ).set_caption( 'PCA Singular Values' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The singular values are used to calculate the variance components.  See the interpretation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Summary Table\n",
    "##\n",
    "## Use a dictionary for the DataFrame columns\n",
    "##\n",
    "data = { 'Singular Value':np.round( pca.singular_values_, 2 ) , \n",
    "         'Variance':np.round( pca.explained_variance_, 2 ),\n",
    "         'Variance(%)':np.round( pca.explained_variance_ratio_*100, 1 ) }\n",
    "df_pca_report = pd.DataFrame( data, index = col_names )\n",
    "##\n",
    "## Add a cumulative sum column\n",
    "##\n",
    "df_pca_report[ 'Cum Sum(%)' ] = df_pca_report[ 'Variance(%)' ].cumsum().round( 1 )\n",
    "##\n",
    "print( 'Number of observations: n = {}'.format( pca.n_samples_ ) )\n",
    "##\n",
    "## Add bar chart to Cum Sum column\n",
    "##\n",
    "df_pca_report.style.bar( subset = [ 'Cum Sum(%)'], align='mid', color = 'red').\\\n",
    "set_caption( 'PCA Summary Table')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The *Variance* column shows the contribution to the total variance of the data and is based on the singular values: $Variance = \\frac{SV^2}{n - 1}$.  The sum of the *Variance* contribution is the total variance.  The first principal component accounts for 25.1% of the variance and the first two account for 50.2%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercises II.A\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "The HR department of a major software company is concerned about a high attrition rate (about 15% each year) among its talented work force.  The sample size for this study is $n = 4410$.  \n",
    "\n",
    "The basis for this problem can be found [here](https://www.kaggle.com/vjchoudhary7/hr-analytics-case-study/activity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise II.A.1 \n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Import the employee data and print the first five records (i.e., the \"head\").  The data are in the HR directory in a CSV file named *employees.csv*.  Call the imported data *df_hr* for consistency with later work.\n",
    "\n",
    "[See Solution](#Solution-II.A.1)\n",
    "\n",
    "The Data Dictionary is:\n",
    "\n",
    "| Variable                  | Values                                       | Source | Mnemonic         |\n",
    "|---------------------------|----------------------------------------------|--------|------------------|\n",
    "| Employee Age              | Nominal Integer                              | HR     | Age              |\n",
    "| Left Company              | Yes/No                                       | HR     | Attrition        | \n",
    "| Amount of Busines Travel  | Non-Travel/Travel_Frequently/Travel_Rarely   | HR     | BusinessTravel   |\n",
    "| Department                | Human Resources/Research & Development/Sales | HR     | Department       |\n",
    "| Distance from Home to Work| Miles                                        | HR     | DistanceFromHome |\n",
    "| Education Level           | Indicator Variable                           | HR     | Education        |\n",
    "| Education Major           | Six Majors as Categorical                    | HR     | EducationField   |\n",
    "| Employee Count            | Just 1 For All Employees                     | HR     | EmployeeCount    |\n",
    "| Employee ID Number        | Nominal Interger                             | HR     | EmployeeID       |\n",
    "| Gender                    | Male/Female                                  | HR     | Gender           |\n",
    "| Job Level                 | Nominal Integer 1 - 5                        | HR     | JobLevel         |\n",
    "| Job Role                  | Nine Categorical Levels                      | HR     | JobRole          |\n",
    "| Martial Status            | Divorced/Married/Single                      | HR     | MaritalStatus    |\n",
    "| Monthly Income            | US\\$                                         | HR     | MonthlyIncome    |\n",
    "| Number of Companies Worked Before| Intger Values (0 = None Before)       | HR     | NumCompaniesWorked |\n",
    "| Over 18 Years Old?        | Y = Yes for All Employees                    | HR     | Over18           |\n",
    "| Percent Salary Increase   | Continuous Whole Number                      | HR     | PercentSalaryHike |\n",
    "| Standard Hours Work/Day   | 8 for All Employees                          | HR     | StandardHours    |\n",
    "| Stock Option Level        | Indicator Variable: 0 - 3                    | HR     | StockOptionLevel |\n",
    "| Total Working Years       | Years Nominal Intergers                      | HR     | TotalWorkingYears |\n",
    "| Number of Training Times Last Year | Days                                | HR     | TrainingTimesLastYear |\n",
    "| Number of Years With Company | Years as Whole Number                     | HR     | YearsAtCompany   |\n",
    "| Number of Years Since Last Promotion |Years as Whole Number (Minimum = 0 | HR     | YearsSinceLastPromotion |\n",
    "| Number of Years with Current Manager | Years as Whole Number (Minimum = 0) | HR   | YearsWithCurrManager |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise II.A.2 \n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Determine the mean and standard deviation of the age of the employees.  Create and interpret a histogram of the age.\n",
    "\n",
    "[See Solution](#Solution-II.A.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for mean and standard deviation\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for histogram\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise II.A.3 \n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Standardize the age to have a zero mean and unit variance.  Determine the mean and standard deviation of the standarized age.  Create and interpret a histogram of the standardized age.\n",
    "\n",
    "[See Solution](#Solution-II.A.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here standardization\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for standardized histogram\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise II.A.4 \n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "The employees belong to three departments: *Human Resources*, *Research \\& Development*, and *Sales*.  The variable is named *Department*.  Determine the proportion of employees in each department, create a barplot of these proportions, and recode the departments with a one-hot ending.\n",
    "\n",
    "[See Solution](#Solution-II.A.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for proportion calculation\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for barplot\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for one-hot encoding\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '038' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson III: Supervised Learning Methods\n",
    "----------------------------------------------------------\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '040' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised vs. Unsupervised Learning\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '042' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '043' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide('044')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '045' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Model (GLM)\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '047' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '048' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test Split Data\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '050' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '051' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '052' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '053' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for Predictive Modeling: Train/Test Split Data\n",
    "\n",
    "The data are split into two parts using *sklearn*'s *train_test_split* function.  Each part has a *X* variable array and a *y* vector (The upper and lower cases are conventional).  The *X* array is a Pandas DataFrame of the *X* variables.  The *y* vector is a Pandas Series.\n",
    "<br><br>\n",
    "Our Case Study data are panel data.  Let's collapse the time dimension and then split on the cross-sectional units.  This means we have to aggregate over the *CID* level.  We have to sum some varioables and find the mean of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Aggregate panel data to CID level for modeling\n",
    "##\n",
    "## Identify variables for modeling and aggregation\n",
    "##\n",
    "x = [ 'CID', 'Region', 'loyaltyProgram', 'buyerRating', 'buyerSatisfaction',\n",
    "      'Usales', 'Pprice', 'Ddisc', 'Odisc', 'Cdisc', 'Pdisc']\n",
    "##\n",
    "## Identify variables for grouping\n",
    "##\n",
    "grp = [ 'CID', 'Region', 'loyaltyProgram', 'buyerRating', 'buyerSatisfaction' ]\n",
    "##\n",
    "## Specify aggregations\n",
    "##\n",
    "aggregations = { 'Usales':'sum', 'Pprice':'mean', 'Ddisc':'mean', 'Odisc':'mean',\n",
    "                 'Cdisc':'mean', 'Pdisc':'mean'}\n",
    "##\n",
    "## Use groupby with agg function to aggregate\n",
    "##\n",
    "tmp = df[ x ].copy()\n",
    "df_agg = tmp.groupby( grp ).agg( aggregations )\n",
    "##\n",
    "## Rename columns and reset index\n",
    "##\n",
    "df_agg.rename( columns = { 'Usales':'totalUsales', 'Pprice':'meanPprice', 'Ddisc':'meanDdisc',\n",
    "                      'Odisc':'meanOdisc', 'Cdisc':'meanCdisc',\n",
    "                      'Pdisc':'meanPdisc'} , inplace = True )\n",
    "df_agg = df_agg.reset_index()\n",
    "##\n",
    "## Print head\n",
    "##\n",
    "df_agg.head().style.set_caption( 'Aggregated data' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "This code block first specifies the data to subset and the types of aggregation to do on the numeric variables.  The aggregations are just *sum* and *mean*.  The *groupby* function does the aggregation by groups specifed as *CID*, *Region*, etc.  The aggregated data are stored in the DataFrame *df_agg*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check the shape of df_agg\n",
    "##\n",
    "print( 'Rows: {}\\nColumns: {}'.format( df_agg.shape[0], df_agg.shape[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "There are 779 unique *CID*s as noted before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create the X and y data for splitting.  Use the aggregated data.\n",
    "##\n",
    "y = df_agg[ 'totalUsales' ]\n",
    "x = [ 'CID', 'Region', 'loyaltyProgram', 'buyerRating', \n",
    "      'buyerSatisfaction', 'meanPprice',\n",
    "      'meanDdisc', 'meanOdisc', 'meanCdisc', 'meanPdisc' ]\n",
    "X = df_agg[ x ]\n",
    "##\n",
    "## Split the data: 1/4 and 3/4.  The default is 3/4 train.\n",
    "##\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.25,\n",
    "                                                    random_state = 42 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The dependent and independent variables need to be separated from the main DataFrame before the train/test split can be done.  The index from the main DataFrame is preserved.  The first three lines of code do this.  The *train_test_split* function randomly divides the data, keeping the indexes aligned.  The *random_state = 42* argument sets the random seed.  Four data sets are returned which are (in order): *X_train, X_test, y_train*, and *y_test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check sample sizes\n",
    "##\n",
    "lst = [ X_train.shape[ 0 ], X_test.shape[ 0 ], y_train.shape[ 0 ], y_test.shape[ 0 ] ]\n",
    "idx = [ 'X_train', 'X_test', 'y_train', 'y_test' ]\n",
    "df_samples_sizes = pd.DataFrame( lst, columns = [ 'n' ], index = idx )\n",
    "df_samples_sizes.style.set_caption( 'Split Sample Sizes' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Display training data\n",
    "##\n",
    "print( 'Sample sizes: \\nTraining X: {}\\nTraining y: {}\\n'.format( X_train.shape[ 0 ], y_train.shape[ 0 ] ) )\n",
    "print( 'Training X Data: \\n{}'.format( X_train.head() ) )\n",
    "print( '\\nTraining y Data: \\n{}'.format( y_train.head() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Display testing data\n",
    "##\n",
    "print( 'Sample sizes: \\nTesting X: {}\\nTesting y: {}\\n'.format( X_test.shape[0], y_test.shape[0] ) )\n",
    "print( 'Testing X Data: \\n{}'.format( X_test.head() ) )\n",
    "print( '\\nTesting y Data: \\n{}'.format( y_test.head() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Note the indexes for the training and testing data sets.  These are the same as the main DataFrame, *df_agg*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the X and y training data for model training.\n",
    "## Do an inner join on the indexes since they match.\n",
    "##\n",
    "## Rename the y variable: totalUsales\n",
    "##\n",
    "yy = pd.DataFrame( { 'totalUsales':y_train } )\n",
    "df_train_ols = yy.merge( X_train, left_index = True, right_index = True )\n",
    "print( 'Complete Training Data Set:\\n{}'.format( df_train_ols.head() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The *X* and *Y* training data sets are merged on the indexes.  Recall that the index were preserved when the *y* and *X* data sets were created.  This is why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the X and y testing data sets for predicting.\n",
    "## Use an inner join on the indexes.\n",
    "##\n",
    "## Rename the y variable totalUsales.\n",
    "##\n",
    "yy = pd.DataFrame( { 'totalUsales':y_test } )\n",
    "df_test_ols = yy.merge( X_test, left_index = True, right_index = True )\n",
    "print( 'Complete Testing Data Set:\\n{}'.format( df_test_ols.head() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Add log Usales and log Pprice to the training data\n",
    "## The log is based on the Numpy function log1p\n",
    "## Note: log1p( x ) = log( 1 + x )\n",
    "##\n",
    "df_train_ols[ 'log_totalUsales' ] = np.log1p( df_train_ols.totalUsales )\n",
    "df_train_ols[ 'log_meanPprice' ] = np.log1p( df_train_ols.meanPprice )\n",
    "print( 'Training Data Set:\\n{}'.format( df_train_ols.head() ) )\n",
    "print( '\\nTraining Data Set Shape:\\n{}'.format( df_train_ols.shape ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "Logged terms are added because Data Visualization showed that logs induce normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Repeat for the testing data\n",
    "##\n",
    "df_test_ols[ 'log_totalUsales' ] = np.log1p( df_test_ols.totalUsales )\n",
    "df_test_ols[ 'log_meanPprice' ] = np.log1p( df_test_ols.meanPprice )\n",
    "##\n",
    "print( 'Testing Data Set:\\n{}'.format( df_test_ols.head() ) )\n",
    "print( '\\nTesting Data Set Shape:\\n {}'.format( df_test_ols.shape ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercises III.A\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise III.A.1 \n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "The employee DataFrame is cross-sectional.  Create training and testing data sets using the default 1/4, 3/4 split.  You will soon model the Percent Salary Hike (*PercentSalaryHike*) each employee last received so use this as the *y* variable.  The *X* variables are *Age*, *Department*, *TotalWorkingYears*, and *YearsAtCompany*.  Call the training set *train_hr* and the testing set *test_hr*, each with the prefix *ols*.\n",
    "\n",
    "[See Solution](#Solution-III.A.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise III.A.2 \n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Merge the training *X* and *y* data sets and then repeat for the two testing data sets.  Call the merged data set *ols_train_hr*.\n",
    "\n",
    "[See Solution](#Solution-III.A.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for training data\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for testing data\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models: Identity Link\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '055' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '056' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## \n",
    "## OLS\n",
    "##\n",
    "## There are four steps for estimatng a model:\n",
    "##\n",
    "##   1. define a formula (i.e., the specific model to estimate)\n",
    "##   2. instantiate the model (i.e., specify it)\n",
    "##   3. fit the model\n",
    "##   4. summarize the fitted model\n",
    "##\n",
    "## ===> Step 1: Define a formula <===\n",
    "##\n",
    "## The formula uses a ~ to separate the left-hand side from the right-hand side\n",
    "## of a model and a + to add columns to the right-hand side.  A - sign (not \n",
    "## used here) can be used to remove columns from the right-hand side (e.g.,\n",
    "## remove or omit the constant term which is always included by default). \n",
    "##\n",
    "formula = 'log_totalUsales ~ log_meanPprice + meanDdisc + meanOdisc +\\\n",
    "    meanCdisc + meanPdisc + C( Region )'\n",
    "##\n",
    "## Since Region is categorical, you must create dummies for the regions. You\n",
    "## do this using 'C( Region )' to indicate that Region is categorical.\n",
    "##\n",
    "## ===> Step 2: Instantiate the OLS model <===\n",
    "##\n",
    "mod = smf.ols( formula, data = df_train_ols )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model <===\n",
    "##      Recommendation: number your models\n",
    "##\n",
    "reg01 = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model <===\n",
    "##\n",
    "print( reg01.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The modeling follows four steps as shown above.  Regardless of the software you might use, these same four steps are followed.  Some software combines them, others require explicit statement.  This is what statsmodels requires.\n",
    "\n",
    "The formula statement is in Patsy format.  The tilde sign (~) replaces an equal sign and the parameters are understood by default.  The Region variable is categorical and, in Patsy, is encoded using the C($\\cdot$) notation. The default encoding for categorical variables is dummy coding or *Treatment* coding in Patsy terminology.  This is indicated, for example, by *C(Region)[T.Northeast]* for the dummy coded *Northeast* region.  Other encodings are possible.  The first level in alphanumeric order is the base and is omitted to avoid the Dummy Variable Trap.  See [here](https://patsy.readthedocs.io/en/latest/overview.html) for documentation on Patsy.  See [here](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) on the Dummy Variable Trap.\n",
    "\n",
    "**_Interpretation_**\n",
    "\n",
    "The price elasticity is the coefficient for the logged price variable (i.e., log_Pprice): -2.8.  If price falls by 1%, unit sales rise by 2.8%.  This indicates that blinds are highly elastic.  This should be expected since furniture is a competitive business and blinds are very competitive.  Revenue will also change.  If price falls, revenue will increase.  The amount revenue will increase (in percentage terms) is given by $1 + elasticity$.  So for a 1% decrease in price, revenue will rise 1.8% (= $1 + [-2.8]$). \n",
    "\n",
    "The discounts seem to have no effect, but this can be tested as we'll do below.  Also note that the adjusted $R^2$ is 0.26 which is very low.  \n",
    "\n",
    "The Jarque-Bera Test is a test for normality of the disturbance term.  It is a test of the \"goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution. $\\ldots$ The null hypothesis is a joint hypothesis of the skewness being zero and the excess kurtosis being zero.  $\\ldots$ If it is far from zero, it signals the data do not have a normal distribution.\"  So the Null Hypothesis is $H_O: Normality$.  (Source: <a href=\"https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test\" target=\"_parent\">see here</a>)  In this case, the Null is rejected.  The Omnibus Test is an alternative test of normality with the same Null.  It also indicates that the Null must be rejected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises III.B\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise III.B.1\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Estimate an *OLS* model for *PercentSalaryHike* regressed on *Age*, *Department*, *TotalWorkingYears*, and *YearsAtCompany*.  Interpret the rsults.\n",
    "\n",
    "**Hint**: *Department* is categorical so you have to create dummies for it.\n",
    "\n",
    "[See Solution](#Solution-III.B.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Analyzing the Results </font>\n",
    "\n",
    "Quantities of interest can be extracted directly from the fitted model. Type *dir(results)* for a full list.\n",
    "\n",
    "Since the product manager wanted to know about a region effect, you should do an F-test of all the coefficients for the regions to determine if they are all zero, meaning that the dummies as a group do nothing.  This is a <u>joint</u> test of significance.  The test statistic is:\n",
    "\n",
    "$F_C = \\dfrac{\\left(SSR_U - SSR_R\\right)/(df_U - df_R)}{SSE_U/(n - p - 1)} = \\dfrac{\\left(SSE_R - SSE_U\\right)/(df_U - df_R)}{SSE_U/(n - p - 1)}$\n",
    "\n",
    "where \"U\" indicates the *unrestricted* or *full* model with the Region dummies and \"R\" indicates the *restricted* model without the Region dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Specify the joint (Null) hypothesis that the regions are the same;\n",
    "## i.e., there is no region effect.\n",
    "##\n",
    "hypothesis = ' ( C(Region)[T.Northeast] = 0, C(Region)[T.South] = 0, \\\n",
    "    C(Region)[T.West] = 0 ) '\n",
    "##\n",
    "## Run and print an F-test for reg01\n",
    "##\n",
    "f_test = reg01.f_test( hypothesis )\n",
    "print( '\\nF (' + str( int( f_test.df_denom ) ) + ', ' + \n",
    "      str( int( f_test.df_num ) ) + ') = ' , \n",
    "      round( f_test.fvalue[0][0], 2 ), '\\np-Value = ', np.round( f_test.pvalue, 4 ) )\n",
    "if f_test.pvalue < 0.05:\n",
    "    print( 'Reject H_0' )\n",
    "else:\n",
    "    print( 'Do not Reject H_0' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "Notice that there are only three regions specified even though there are four: one is omitted as the base.  Also notice that the three hypotheses are specified as *C(Region)[T.XX] = 0* where *XX* is the region name.\n",
    "\n",
    "**_Output Interpretation_**\n",
    "\n",
    "The returned values for the F-test are, in order:\n",
    "\n",
    "1. The F-Statistic value\n",
    "2. The p-value for the F-Statistic\n",
    "3. The F-Statistic's denominator degrees-of-freedom\n",
    "4. The F-Statistic's numerator degrees-of-freedom\n",
    "\n",
    "**_Interpetation_**\n",
    "\n",
    "The Null Hypothesis is that there is no region effect.  The p-value is 0.0 so the Null Hypothesis is rejected: there is a Region effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Repeat the F-test for the discounts\n",
    "##\n",
    "hypothesis = ' ( meanDdisc = 0, meanOdisc = 0, meanCdisc = 0, meanPdisc = 0 ) '\n",
    "##\n",
    "## Run and print an F-test for reg01\n",
    "##\n",
    "f_test = reg01.f_test( hypothesis )\n",
    "print( '\\nF (' + str( int( f_test.df_denom ) ) + ', ' + \n",
    "      str( int( f_test.df_num ) ) + ') = ' , \n",
    "      round( f_test.fvalue[0][0], 2 ), '\\np-Value = ', np.round( f_test.pvalue, 4 ) )\n",
    "if f_test.pvalue < 0.05:\n",
    "    print( 'Reject H_0' )\n",
    "else:\n",
    "    print( 'Do not Reject H_0' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The hypothesis statement does not have the discount names as *C(Ddis)* etc. because they are quantitative variables, not categorical variables like *Region*. \n",
    "\n",
    "The Null Hypothesis is that there is no difference among the discounts; they all have zero effect on unit sales.  Notice that the p-value is 0.20.  So the Null Hypothesis that the discounts all have the same effect is not rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for multicollinearity -- a linear relationship among the variables.  You can use the corrlation matrix but this is only good for pair-wise relationships.  The *variance inflation factor* (*VIF*) is better.  A rule-of-thumb is that any $VIF > 10$ indicates a problem.  See [here](https://en.wikipedia.org/wiki/Variance_inflation_factor) on the *VIF*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Subset the design matrix to eliminate the first column of 1s\n",
    "## the iloc method says to find the location of columns based on \n",
    "## their integer locations (i.e., 0, 1, 2, etc.)\n",
    "## the term in brackets says to find all rows (the : ) and all \n",
    "## columns from the first to the end (1: )\n",
    "##\n",
    "## Create the correlation matrix\n",
    "##\n",
    "x = reg01.model.data.orig_exog.iloc[ :, 4: ] \n",
    "corr_matrix = x.corr()\n",
    "corr_matrix.style.format( '{:,.2f}' ).set_caption( 'Correlation Matrix' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate VIFs\n",
    "## The VIFs are the diagonal elements of the inverted correlation\n",
    "## matrix of the independent variables.\n",
    "##\n",
    "## Subset the design matrix to eliminate the first column of 1s.\n",
    "## The iloc method says to find the location of columns based on their \n",
    "## integer locations (i.e., 0, 1, 2, etc.) the term in brackets says \n",
    "## to find all rows (the : ) and all columns from the first to the end (1: ).\n",
    "##\n",
    "## Create the correlation matrix\n",
    "##\n",
    "x = reg01.model.data.orig_exog.iloc[ :, 1: ]\n",
    "corr_matrix = x.corr()\n",
    "##\n",
    "## Invert the correlation matrix and extract the main diagonal\n",
    "##\n",
    "vif = np.diag( np.linalg.inv( corr_matrix ) ) \n",
    "##\n",
    "## Zip the variable names and the VIFs\n",
    "##\n",
    "indepvars = [ i for i in x.columns ]\n",
    "xzip = zip( indepvars, np.round( vif, 1 ) ) \n",
    "##\n",
    "## Display the zip matrix.\n",
    "##\n",
    "df_vif = pd.DataFrame( xzip, columns = [ 'Variable', 'VIF' ] )\n",
    "df_vif.set_index( 'Variable', inplace = True )\n",
    "df_vif.style.applymap( lambda x: 'background-color : yellow' if x > 10 else '' ).set_caption( 'VIF Values' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "All but two *VIF*s are below 10 so there is little problem.  $VIF > 10$ is a rule-of-thumb for indicating multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Predicting with the Model </font>\n",
    "\n",
    "Predict unit sales.  Recognize that sales are in (natural) log terms.  They need to be converted back to unit sales in \"normal\" terms by exponentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calculate predicted log of unit sales, the dependent variable.\n",
    "##\n",
    "## Note: the inverse of the log is needed; use np.expm1( x )\n",
    "## since log1p was used: np.expm1 = exp(x) - 1.\n",
    "##\n",
    "log_pred = reg01.predict( df_test_ols )\n",
    "y_pred = np.expm1( log_pred )\n",
    "##\n",
    "## Combine into one temporary DataFrame for convenience\n",
    "##\n",
    "tmp = pd.DataFrame( { 'y_test':y_test, 'y_logPred':log_pred, 'y_pred':y_pred } )\n",
    "tmp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the sklearn metrics function *r2_score* to check the fit of actual vs. predicted values.  From the sklearn [User Guide](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score):\n",
    "\n",
    "\"*The r2_score function computes $R^2$, the coefficient of determination. ... It provides a measure of how well future samples are likely to be predicted by the model. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a $R^2$ score of 0.0.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Display the r2 score.  But first drop any NaN data.\n",
    "##\n",
    "tmp.dropna( inplace = True )\n",
    "print( 'r2 Score:\\n {}'. format( round( r2_score( tmp.y_test, tmp.y_pred), 3 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The 0.10 is not very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict unit sales for different settings of the variables.  This is *scenario* or *what-if* analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Specify scenario values to use for prediction\n",
    "##\n",
    "## Create a dictionary\n",
    "##\n",
    "data = {\n",
    "         'meanPprice': [ 2.50 ],\n",
    "         'meanDdisc': [ 0.03 ],\n",
    "         'meanOdisc': [ 0.05 ],\n",
    "         'meanCdisc': [ 0.03 ],\n",
    "         'meanPdisc': [ 0.03 ],\n",
    "         'Region': [ 'West' ]\n",
    "        }\n",
    "##\n",
    "## Create a DataFrame using the dictionary\n",
    "##\n",
    "df_scenario = pd.DataFrame.from_dict( data )\n",
    "##\n",
    "## Insert a log price column after the Pprice variable\n",
    "##\n",
    "df_scenario.insert( loc = 1, column = 'log_meanPprice',\n",
    "                value = np.log1p( df_scenario.meanPprice ) )\n",
    "##\n",
    "## Display the settings and the predicted unit sales\n",
    "##\n",
    "print( 'Scenario settings:\\n{}'.format( df_scenario ) )\n",
    "##\n",
    "## Create a prediction\n",
    "##\n",
    "log_pred = reg01.predict( df_scenario )\n",
    "y_pred = np.expm1( log_pred )\n",
    "print( '\\nPredicted Unit Sales: \\n{}'.format( round( y_pred, 0 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression: Logit Link\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '059' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide('060' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '061' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '062' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = black> Create your Data </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer satisfaction is part of the DataFrame.  Satisfaction is measured on a five-point scale: *1 = Not at All Satisfied*, *5 = Very Satisfied*.  \n",
    "\n",
    "First, look at the frquency count of satisfaction.  But, there is a problem: you cannot use the same data as before since satisfaction is by customer and the data used so far are by transaction.  The satisfaction rating is in the *df_agg* DataFrame.  So, there are three steps:\n",
    "\n",
    "1. Recode the scale values in the merged file so that 1 is the top-two values (called *top-two box* or *T2B*) and 0 is all other values.  The *T2B* is *Very Satisfied*.\n",
    "2. Split the data into training and testing datasets.\n",
    "3. Train a model with *T2B* satisfaction as a function of the pocket price, discounts, and Region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 1: Recode the scale values so that 1 is the top-two values <===\n",
    "## (called \"top-two box\" or \"T2B\") and 0 is all other values.  \n",
    "## The \"T2B\" is \"Very Satisfied\".\n",
    "##\n",
    "## Define a lambda function for the recoding\n",
    "##\n",
    "df_agg[ 'sat_t2b' ] = df_agg.buyerSatisfaction.map( lambda x: 1 if ( x >= 4 ) else 0 )\n",
    "##\n",
    "x = df_agg[ 'sat_t2b' ].value_counts( normalize = True ).round( 3 )\n",
    "x = pd.DataFrame( x )\n",
    "x.rename( index = { 1:'T2B', 0:'B3B' }, inplace = True )\n",
    "x.style.set_caption( 'Proportion in each box' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "\n",
    "The *rename* method sets the values for the index using a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model *T2B* satisfaction as a function of the pocket price and discounts.  First, create training and testing DataFrames as before but with *sat_t2b* as the *y* variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 2: Split the data. <===\n",
    "##\n",
    "## ===> Step 2.A: Create the X and y data for splitting <===\n",
    "##\n",
    "y = df_agg[ 'sat_t2b' ]\n",
    "x = [ 'meanPprice', 'meanDdisc', 'meanOdisc', 'meanCdisc', 'meanPdisc', 'Region', \n",
    "     'loyaltyProgram', 'buyerRating' ]\n",
    "X = df_agg[ x ]\n",
    "##\n",
    "## ===> Step 2.B: Split the data: 1/3 and 2/3. <===\n",
    "##\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.33, \n",
    "                                                    random_state = 42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Display some data\n",
    "##\n",
    "print( 'X Training Data: \\n{}'.format( X_train.head() ) ) \n",
    "print( '\\nY Training Data: \\n{}'.format( y_train.head() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the two training sets for convenience\n",
    "##\n",
    "yy = pd.DataFrame( { 'sat_t2b':y_train } )\n",
    "df_train_logit = yy.merge( X_train, left_index = True, right_index = True )\n",
    "df_train_logit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check counts\n",
    "##\n",
    "print( 'Number of rows: {rows}\\nNumber of columns: {cols}'.format( \n",
    "        rows = df_train_logit.shape[0], cols = df_train_logit.shape[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice that there are 521 training records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the two testing sets for convenience\n",
    "##\n",
    "yy = pd.DataFrame( { 'sat_t2b':y_test } )\n",
    "df_test_logit = yy.merge( X_test, left_index = True, right_index = True )\n",
    "df_test_logit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Check counts\n",
    "##\n",
    "print( 'Number of rows: {rows}\\nNumber of columns: {cols}'.format( \n",
    "        rows = df_test_logit.shape[0], cols = df_test_logit.shape[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice that there are 258 testing records.  The total of train + test is 521 + 258 = 779 which we had before for *df_agg*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color = black> Train a Model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 3: Train a logit model <===\n",
    "##\n",
    "## ===> Step 3A: Define a formula <===\n",
    "##\n",
    "formula = 'sat_t2b ~ meanPprice + meanDdisc + meanOdisc + meanCdisc + meanPdisc + C( Region )'\n",
    "##\n",
    "## ===> Step #b: Instantiate the logit model <===\n",
    "##\n",
    "mod = smf.logit( formula, data = df_train_logit )\n",
    "##\n",
    "## ===> Step 3C: Fit the instantiated model <===\n",
    "##\n",
    "logit01 = mod.fit()\n",
    "##\n",
    "## ===> Step 3D: Summarize the fitted model <===\n",
    "##\n",
    "print( logit01.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Predicting with the Model </font>\n",
    "\n",
    "The prediction process is the same as discussed for *Case I* above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises III.C\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise III.C.1\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "The employee DataFrame is cross-sectional.  Create training and testing data sets using the default 1/4, 3/4 split.  You will soon model the attrition (*Attrition*) so use this as the *y* variable.  The *X* variables are *Age*, *Department*, *TotalWorkingYears*, *YearsAtCompany*, and *YearsSinceLastPromotion*.  Call the training set *train_hr* and the testing set *test_hr*, each with the prefix *logit*.\n",
    "\n",
    "[See Solution](#Solution-III.C.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise III.C.2\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Merge the data sets.\n",
    "\n",
    "[See Solution](#Solution-III.C.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for merging the training data\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for merging the testing data\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise III.C.3\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Train a logit model.\n",
    "\n",
    "[See Solution](#Solution-III.C.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Enter code here for training a logit model\n",
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '065' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '066' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '068' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '069' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '070' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '071' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '072' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '073' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## NB can only handle numeric data, so Region, loyaltyProgram, and\n",
    "## buyerRating must be encoded using labelEncoder -- see package \n",
    "## load section above.\n",
    "##\n",
    "df_train_nb = df_train_logit.copy()\n",
    "df_train_nb[ 'Region' ] = le.fit_transform( df_train_nb.Region )\n",
    "df_train_nb[ 'loyaltyProgram' ] = le.fit_transform( df_train_nb.loyaltyProgram )\n",
    "df_train_nb[ 'buyerRating' ] = le.fit_transform( df_train_nb.buyerRating )\n",
    "df_train_nb.head().style.set_caption( 'Training Data' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## NB requires a separate X and y data set, so separate out\n",
    "## y = sat_t2b and X\n",
    "##\n",
    "y_train = df_train_nb[ 'sat_t2b' ]\n",
    "x = y_train.value_counts( normalize = True ).round( 3 )\n",
    "x = pd.DataFrame( x )\n",
    "x.rename( index = { 1:'T2B', 0:'B3B' }, inplace = True )\n",
    "x.style.set_caption( 'Proportion in each box' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Separate X variables\n",
    "##\n",
    "x = [ 'meanPprice', 'meanDdisc', 'meanOdisc', 'meanCdisc', 'meanPdisc',\n",
    "       'Region', 'loyaltyProgram', 'buyerRating' ]\n",
    "X_train = df_train_nb[ x ]\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Repeat for testing data\n",
    "##\n",
    "df_test_nb = df_test_logit.copy()\n",
    "df_test_nb[ 'Region' ] = le.fit_transform( df_test_nb.Region )\n",
    "df_test_nb[ 'loyaltyProgram' ] = le.fit_transform( df_test_nb.loyaltyProgram )\n",
    "df_test_nb[ 'buyerRating' ] = le.fit_transform( df_test_nb.buyerRating )\n",
    "df_test_nb.head().style.set_caption( 'Testing Data' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Repeat for testing y data\n",
    "##\n",
    "y_test = df_test_nb[ 'sat_t2b' ]\n",
    "x = y_test.value_counts( normalize = True ).round( 3 )\n",
    "x = pd.DataFrame( x )\n",
    "x.rename( index = { 1:'T2B', 0:'B3B' }, inplace = True )\n",
    "x.style.set_caption( 'Proportion in each box' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Separate X variables\n",
    "##\n",
    "x = [ 'meanPprice', 'meanDdisc', 'meanOdisc', 'meanCdisc', 'meanPdisc',\n",
    "       'Region', 'loyaltyProgram', 'buyerRating' ]\n",
    "X_test = df_test_nb[ x ]\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 1: Instantiate a Gaussian Classifier <===\n",
    "##\n",
    "gnb = GaussianNB()\n",
    "##\n",
    "## === Step 2: Train the model using the training sets <===\n",
    "##\n",
    "gnb.fit(X_train, y_train)\n",
    "##\n",
    "## ===> Step 3: Predict the response for test dataset <===\n",
    "##\n",
    "y_pred = gnb.predict( X_test )\n",
    "##\n",
    "## ===> Step 4: Print predicted values <===\n",
    "##\n",
    "print( \"Predicted Value:\\n\", y_pred )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Categorize as Over/Under/Equal on prediction\n",
    "##\n",
    "data = { 'test':y_test, 'predicted':y_pred }\n",
    "df_gnb = pd.DataFrame( data )\n",
    "##\n",
    "## Define a lambda function for recoding\n",
    "##\n",
    "df_gnb[ 'Over/Under' ] = df_gnb.apply( lambda x: 'Over' if ( x.predicted > x.test ) else\n",
    "                                      ( 'Under' if ( x.predicted < x.test ) else 'Equal' ), axis = 1 )\n",
    "##\n",
    "df_gnb.head().style.set_caption( 'Over/Under/Equal Categorization' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Prediction distribution\n",
    "##\n",
    "df_gnb[ 'Over/Under' ].value_counts( normalize = True ).round( 4 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create simple barchart\n",
    "##\n",
    "y = df_gnb[ 'Over/Under' ].value_counts( normalize = True )\n",
    "ax = sns.barplot( y = y, x = [ 'Equal', 'Over', 'Under' ] )\n",
    "ax.set( title = 'Naive Bayes Prediction Accuracy' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Model Accuracy, how often is the classifier correct?\n",
    "##\n",
    "print(\"Accuracy: {}\".format( round( metrics.accuracy_score( y_test, y_pred ), 3) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice that the accuracy measure is the same as the \"Equal\" group above.  They should be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '076' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '077' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '078' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '079' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '080' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the *X_train* and *y_train* data from the Naive Bayes.  Recall that *y_train* is the *sat_t2b\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## ===> Step 1: Create a SVM Classifier <===\n",
    "## Notice that SVC is used for the classifier\n",
    "##\n",
    "svm = svm.SVC( kernel = 'linear' ) # Linear Kernel\n",
    "##\n",
    "## ===> Step 2: Instantiate the model using the training sets <===\n",
    "##\n",
    "svm.fit( X_train, y_train )\n",
    "##\n",
    "## ===> Step 3: Predict the response for test dataset <===\n",
    "##\n",
    "y_pred = svm.predict( X_test )\n",
    "##\n",
    "## ===> Step 4: Model Accuracy: How often is the classifier correct? <===\n",
    "##\n",
    "print(\"Accuracy: {}\".format( round( metrics.accuracy_score( y_test, y_pred ), 3 ) ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "The *SVM* classifier is slightly more accurate than the logit model: 0.682 for SVM; 0.628 for logit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '083' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '084' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '085' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '086' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '087' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '088' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Note: The Region variable was previously encoded as:\n",
    "##\n",
    "##          0: Midwest\n",
    "##          1: Northeast\n",
    "##          2: South\n",
    "##          3: West\n",
    "##\n",
    "## ===> Step 1: Instantiate the tree <===\n",
    "##\n",
    "dtree = tree.DecisionTreeClassifier( random_state = 0, max_depth = 3, \n",
    "                                    min_samples_leaf = 10 )\n",
    "##\n",
    "## ===> Step 2: Fit the tree <===\n",
    "##\n",
    "dtree.fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Displaying a tree is a slight challenge!\n",
    "## There are four steps:\n",
    "##\n",
    "## ===> Step 1: Create a placeholder for all the plotting points. <===\n",
    "##\n",
    "dot_data = StringIO()\n",
    "##\n",
    "## ===> Step 2: Extract the feature names for labels models. <===\n",
    "##\n",
    "feature_names = [ i for i in X_train.columns ]\n",
    "print( 'Feature names for tree:\\n{}'.format( feature_names ) )\n",
    "##\n",
    "## ===> Step 3: Export the plotting data to the placeholder. <===\n",
    "##\n",
    "export_graphviz(dtree, out_file = dot_data,  \n",
    "                filled = True,\n",
    "                rounded = True,\n",
    "                special_characters = True,\n",
    "                class_names = [ 'B3B Sat', 'T2B Sat' ],  ## Order is 0 then 1\n",
    "                feature_names = feature_names,\n",
    "                proportion = True\n",
    "               )\n",
    "##\n",
    "## ===> Step 4: Create the display. <===\n",
    "##\n",
    "graph = pydotplus.graph_from_dot_data( dot_data.getvalue() )\n",
    "Image( graph.create_png() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '090' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson IV: Unsupervised Learning Methods\n",
    "----------------------------------------------------------\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '092' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types and Characteristics of Unsupervised Learning\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '094' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '096' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '098' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '099' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '100' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Subset the df_agg data\n",
    "##\n",
    "x = [ 'Region', 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "df_hclusters = df_agg[ x ]\n",
    "df_hclusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Standardize the six numeric variables.\n",
    "##\n",
    "x = [ 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "##\n",
    "## Extract the variables as a temporary DataFrame\n",
    "##\n",
    "tmp = df_hclusters[ x ]\n",
    "##\n",
    "## Standardize and horizontally concatenate with the Region variable; \n",
    "## concatenate on axis = 1\n",
    "##\n",
    "tmp_standard = pd.DataFrame( StandardScaler().fit_transform( tmp ), columns = x )\n",
    "df_hclusters = pd.concat( [ df_hclusters[ 'Region'], tmp_standard ], axis = 1 )\n",
    "df_hclusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Convert character labels to numerics\n",
    "##\n",
    "x = le.fit_transform( df_hclusters.Region )\n",
    "df_hclusters[ 'Region' ] = x\n",
    "df_hclusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use Ward's minimum variance linkage method\n",
    "##\n",
    "ward = shc.linkage( df_hclusters, method = 'ward' )\n",
    "##\n",
    "## Plot a dendogram\n",
    "## WARNING: this will take a minute\n",
    "##\n",
    "max_dist = 23\n",
    "##\n",
    "plt.figure( figsize = ( 10, 7  ) )  \n",
    "plt.title( 'CID Clustering\\nHierarchical Clustering Dendrogram\\nWard\\'s Method' )\n",
    "plt.xlabel( 'Customer (CID)' )\n",
    "plt.ylabel( 'Distance' )\n",
    "shc.dendrogram( ward )\n",
    "plt.axhline( y = max_dist, c = 'black', ls = '-', lw = 1.5 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "A horizontal line is drawn at a distance of 23.  Any clusters formed below this line is a group.  Also notice that there are four groups.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Identify the CIDs in each cluster\n",
    "## Consider any cluster grouping formed below 23\n",
    "##\n",
    "cluster_labels = fcluster( ward, max_dist, criterion = 'distance' )\n",
    "df_hclusters[ 'Cluster_Number' ] = cluster_labels\n",
    "df_hclusters.head().style.set_caption( 'DataFrame with cluster number added' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Examine the cluster size distribution\n",
    "##\n",
    "tmp = df_hclusters.Cluster_Number.value_counts( normalize = True )\n",
    "df_hcluster_sizes = tmp.to_frame()\n",
    "df_hcluster_sizes.index = [ 'Cluster ' + str( x ) for x in tmp.index ]\n",
    "df_hcluster_sizes.style.format( '{:0.3f}' ).set_caption( 'Proportion of sample in each cluster' ) \\\n",
    ".bar( subset = [ 'Cluster_Number'], align='mid', color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Bar chart of cluster sizes\n",
    "##\n",
    "y = df_hcluster_sizes.Cluster_Number\n",
    "idx = df_hcluster_sizes.index\n",
    "ax = sns.barplot( y = y, x = idx )\n",
    "ax.set( title = 'Hierarchical Clustering' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create a boxplot for each cluster for Order Discount\n",
    "##\n",
    "ax = sns.boxplot( x = 'Cluster_Number', y = 'meanOdisc', data = df_hclusters )\n",
    "ax.set( title = 'Order Discount\\nby Clusters\\nHierarchical Clustering', xlabel = 'Clusters',\n",
    "      ylabel = 'Order Discount' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercises IV.A\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise IV.A.1\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Create a hierarchical clustering using the following variables:\n",
    "\n",
    "1. Age\n",
    "2. MonthlyIncome\n",
    "3. PercentSalaryHike\n",
    "4. TotalWorkingYears\n",
    "5. YearsAtCompany\n",
    "\n",
    "HINT 1: The first step is to standardize these five variables.\n",
    "\n",
    "HINT 2: The variable *TotalWorkingYears* has missing values.\n",
    "\n",
    "[See Solution](#Solution-IV.A.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '103' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Set up data \n",
    "##\n",
    "x = [ 'Region', 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "df_kclusters = df_agg[ x ].copy()\n",
    "df_kclusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Subset the data for all numerics\n",
    "##\n",
    "x = [ 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "tmp = df_kclusters[ x ]\n",
    "##\n",
    "## Do KMeans\n",
    "##\n",
    "kmeans = KMeans( n_clusters = 4, random_state = 1234 ).fit( tmp )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The kmeans centers are retrieved using the method *cluster_centers_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## \n",
    "## Add cluster labels to main cluster DataFrame\n",
    "##\n",
    "df_kclusters[ 'Cluster_Number' ] = kmeans.labels_   ## Notice the underscore\n",
    "df_kclusters.head().style.set_caption( 'DataFrame with k-Means cluster numbers' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Code Explanation_**\n",
    "\n",
    "The kmeans cluster numbers are retrieved using the method *labels_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Examine the cluster size distribution\n",
    "##\n",
    "tmp = df_kclusters.Cluster_Number.value_counts( normalize = True )\n",
    "df_kcluster_sizes = tmp.to_frame()\n",
    "df_kcluster_sizes.index = [ 'Cluster ' + str( x ) for x in tmp.index ]\n",
    "df_kcluster_sizes.style.format( '{:0.3f}' ).set_caption( 'Proportion of sample in each cluster' ) \\\n",
    ".bar( subset = [ 'Cluster_Number'], align='mid', color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Bar chart of cluster sizes\n",
    "##\n",
    "y = df_kcluster_sizes.Cluster_Number\n",
    "idx = df_kcluster_sizes.index\n",
    "ax = sns.barplot( y = y, x = idx )\n",
    "ax.set( title = 'KMeans Clustering' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Boxplot of Order Discount\n",
    "##\n",
    "ax = sns.boxplot( y = 'meanOdisc', x = 'Cluster_Number', data = df_kclusters )\n",
    "ax.set( title = 'Mean Order Discount\\nby Clusters\\nKMeans Clustering', ylabel = 'Order Discount' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixture Models\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '106' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '107' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '108' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Models\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Set up data \n",
    "##\n",
    "x = [ 'Region', 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "df_mclusters = df_agg[ x ].copy()\n",
    "##\n",
    "## sub\n",
    "x = [ 'totalUsales', 'meanPprice', 'meanDdisc', 'meanCdisc', 'meanPdisc', 'meanOdisc' ]\n",
    "tmp = df_mclusters[ x ] \n",
    "##\n",
    "## ===> Step 1: Instantiate the model <===\n",
    "##\n",
    "gmm = GaussianMixture( n_components = 4 ) \n",
    "##  \n",
    "## ===> Step 2: Fit the model <===\n",
    "##\n",
    "gmm.fit( tmp ) \n",
    "##  \n",
    "## ===> Step 3: Add cluster labels to main cluster DataFrame <===\n",
    "##\n",
    "cluster_number = gmm.predict( tmp ) \n",
    "df_mclusters[ 'Cluster_Number'] = cluster_number\n",
    "df_mclusters.head().style.set_caption( 'DataFrame with mixture cluster numbers' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Examine the cluster size distribution\n",
    "##\n",
    "tmp = df_mclusters.Cluster_Number.value_counts( normalize = True )\n",
    "df_mcluster_sizes = tmp.to_frame()\n",
    "df_mcluster_sizes.index = [ 'Cluster ' + str( x ) for x in tmp.index ]\n",
    "df_mcluster_sizes.style.format( '{:0.3f}' ).set_caption( 'Proportion of sample in each cluster' ) \\\n",
    ".bar( subset = [ 'Cluster_Number'], align='mid', color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Bar chart of cluster sizes\n",
    "##\n",
    "y = df_mclusters[ 'Cluster_Number' ].value_counts( normalize = True )\n",
    "idx = df_mcluster_sizes.index\n",
    "ax = sns.barplot( y = y, x = idx )\n",
    "ax.set( title = 'Gaussian Mixture Clustering' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Boxplot of Order Discount\n",
    "##\n",
    "ax = sns.boxplot( y = 'meanOdisc', x = 'Cluster_Number', data = df_mclusters )\n",
    "ax.set( title = 'Mean Order Discount\\nby Clusters\\nGaussian Mixture Clustering', \n",
    "       ylabel = 'Order Discount', xlabel = 'Cluster' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '110' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson V: Model Evaluation\n",
    "----------------------------------------\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '112' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '113' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '114' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '115' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '116' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '117' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = black> Analyze the Logit Modeling Results </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Make predictions\n",
    "## Use logit_test from logit section above\n",
    "##\n",
    "predictions = logit01.predict( df_test_logit )\n",
    "predictions_nominal = [ 0 if x < 0.5 else 1 for x in predictions ]\n",
    "x = classification_report( y_test, predictions_nominal, digits = 3 )\n",
    "print( 'Logit Model Classification Report:\\n{}'.format( x ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "=====================\n",
    "\n",
    "To quote from [here](https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8):\n",
    "\n",
    "The precision is the ratio $tp/(tp + fp)$ where $tp$ is the number of true positives and $fp$ the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\n",
    "\n",
    "The recall is the ratio $tp/(tp + fn)$ where $tp$ is the number of true positives and $fn$ the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
    "\n",
    "The F-beta score weights the recall more than the precision by a factor of beta. beta = 1.0 means recall and precision are equally important.\n",
    "\n",
    "The support is the number of occurrences of each class in y_test.\n",
    "\n",
    "Also see [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification, the count of **true negatives** ($tn$), **false negatives** ($fn$), **true positives** ($tp$), and **false positives** ($fp$) can be found from a *confusion matrix*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create a confusion matrix\n",
    "##\n",
    "x = confusion_matrix(y_test, predictions_nominal).ravel()\n",
    "##\n",
    "## zip the variable names and the confusion\n",
    "##\n",
    "lbl = [ 'True Negative', 'False Positive', 'False Negative', 'True Positive' ]\n",
    "##\n",
    "## display the confusion matrix in a DataFrame\n",
    "##\n",
    "df_confusion = pd.DataFrame( x, columns = [ 'Value' ], index = lbl )\n",
    "df_confusion[ 'Proportion (%)' ] = df_confusion.Value/df_confusion.Value.sum()\n",
    "df_confusion.style.format( { 'Proportion (%)': '{:.1%}' } ).highlight_max( color = 'yellow' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "There was 1 true negative, 81 false positives, 3 false negatives, and 173 true positives.  The total is 258 which is the size of the testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot the confusion values\n",
    "##\n",
    "ax = sns.barplot( y = df_confusion.index, x = df_confusion[ 'Proportion (%)' ] )\n",
    "ax.set( title = 'Percent of Sample\\nby Confusion Labels',\n",
    "        xlabel = 'Percent Confusion', ylabel = '' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative plot of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create labels\n",
    "##\n",
    "lbl = ['Not Satisfied', 'Satisfied']\n",
    "##\n",
    "## Create the confusion matrix\n",
    "##\n",
    "cm = confusion_matrix( y_test, predictions_nominal )\n",
    "df_cm = pd.DataFrame( data = cm/cm.sum(), index = lbl, columns = lbl )\n",
    "df_cm.style.format( { 'Not Satisfied': '{:.1%}', 'Satisfied': '{:.1%}' } ).highlight_max( color = 'yellow' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "67\\% of the cases were predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contact Information\n",
    "----------------------------\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "If you have any questions after this course, please do not hesitate to contact me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide( '120' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solutions\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution II.A.1\n",
    "\n",
    "[Return to Exercise II.A.1](#Exercise-II.A.1)\n",
    "\n",
    "Import the employee data and print the first five records (i.e., the \"head\"). The data are in a CSV file named employees.csv. Call the imported data *df_hr* for consistency with later work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use the previously defined data path\n",
    "##\n",
    "df_hr = pd.read_csv( path + 'employees.csv' )\n",
    "df_hr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution II.A.2\n",
    "\n",
    "[Return to Exercise II.A.2](#Exercise-II.A.2)\n",
    "\n",
    "Determine the mean and standard deviation of the age of the employees.  Create and interpret a histogram of the age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Extract the Age variable and print to 1 decimal place\n",
    "##\n",
    "x = df_hr.Age\n",
    "print( 'Mean of Age: {0:0.1f}\\nStandard Deviation of Age: {1:0.1f}'.format( x.mean(), x.std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Alternative\n",
    "##\n",
    "x.describe().round( 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Use the Seaborn distplot function for a histogram\n",
    "##\n",
    "ax = sns.distplot( x )\n",
    "ax.set( title = 'Histogram of Age', xlabel = 'Employee Age', ylabel = 'Proportion' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution II.A.3\n",
    "\n",
    "[Return to Exercise II.A.3](#Exercise-II.A.3)\n",
    "\n",
    "Standardize the age to have a zero mean and unit variance.  Determine the mean and standard deviation of the standarized age.  Create and interpret a histogram of the standardized age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Standardize Age\n",
    "## Use preprocessor -- see package loading section\n",
    "##\n",
    "x = df_hr.Age\n",
    "x_standard = pp.scale( x )\n",
    "print( 'Mean of Standardized Age: {0:0.1f}\\nStandard Deviation of Standardized Age: {1:0.1f}'.\n",
    "      format( x_standard.mean(), x_standard.std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Plot histogram of standardized Age\n",
    "##\n",
    "ax = sns.distplot( x_standard )\n",
    "ax.set( title = 'Histogram of Standardized Age', xlabel = 'Age', ylabel = 'Proportion' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Interpretation_**\n",
    "\n",
    "Notice that this distribution is centered at 0.0 as it should be since the mean was removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution II.A.4\n",
    "\n",
    "[Return to Exercise II.A.4](#Exercise-II.A.4)\n",
    "\n",
    "The employees belong to three departments: *Human Resources*, *Research \\& Development*, and *Sales*.  The variable is named *Department*.  Determine the proportion of employees in each department, create a barplot of these proportions, and recode the departments with a one-hot ending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use *value_counts* with the *normalize = True* argument\n",
    "##\n",
    "x = df_hr.Department\n",
    "x.value_counts( normalize = True ).round( 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Use the Seaborn barplot function\n",
    "##\n",
    "y = x.value_counts(normalize = True)\n",
    "ax = sns.barplot( y = y, x = y.index )\n",
    "ax.set( title = 'Department Distribution' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Alternative barplot display -- this is better\n",
    "##\n",
    "y = x.value_counts(normalize = True)\n",
    "ax = sns.barplot( y = y.index, x = y )\n",
    "ax.set( title = 'Department Distribution' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## one-hot encoding\n",
    "##\n",
    "enc = pp.OneHotEncoder()  ## Order is alphanumeric so HR is first\n",
    "tmp = df_hr[ ['Department' ] ]\n",
    "ohe = enc.fit_transform( tmp ).toarray()\n",
    "## \n",
    "## Create a DataFrame for easier viewing\n",
    "##\n",
    "df_tmp = pd.DataFrame( ohe, columns = [ \"HR\", \"R\\&D\", \"Sales\" ] )  ## Note the alphanumeric order\n",
    "df_ohe = pd.concat( [ df_hr.Department, df_tmp ], axis=1 )\n",
    "df_ohe.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution III.A.1\n",
    "\n",
    "[Return to Exercise III.A.1](#Exercise-III.A.1)\n",
    "\n",
    "The employee DataFrame is cross-sectional.  Create training and testing data sets using the default 1/4, 3/4 split.  You will soon model the Percent Salary Hike (*PercentSalaryHike*) each employee last received so use this as the *y* variable.  The *X* variables are *Age*, *Department*, *TotalWorkingYears*, and *YearsAtCompany*.  Call the training set *train_hr* and the testing set *test_hr*, each with the prefix *ols*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Create the X and y data for splitting.  Use the entire HR data.\n",
    "##\n",
    "y = df_hr[ 'PercentSalaryHike' ]\n",
    "x = [ 'Age', 'Department', 'TotalWorkingYears', 'YearsAtCompany' ]\n",
    "X = df_hr[ x ]\n",
    "##\n",
    "## Split the data: 1/4 and 3/4.  The default is 3/4 train.\n",
    "##\n",
    "X_train_hr, X_test_hr, y_train_hr, y_test_hr = train_test_split( X, y, test_size = 0.25,\n",
    "                                                    random_state = 42 )\n",
    "X_train_hr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution III.A.2\n",
    "\n",
    "[Return to Exercise III.A.2](#Exercise-III.A.2)\n",
    "\n",
    "Merge the training *X* and *y* data sets and then repeat for the two testing data sets.  Call the merged data set *ols_train_hr*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the X and y training data for \n",
    "## model training.  Do an inner join on the indexes.\n",
    "##\n",
    "## Rename the y variable: PercentSalaryHike\n",
    "##\n",
    "yy = pd.DataFrame( { 'PercentSalaryHike':y_train_hr } )\n",
    "ols_train_hr = yy.merge( X_train_hr, left_index = True, right_index = True )\n",
    "print( 'Training Data Set:\\n\\n{}'.format( ols_train_hr.head() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the X and y testing data for \n",
    "## model testing.  Do an inner join on the indexes.\n",
    "##\n",
    "## Rename the y variable: PercentSalaryHike\n",
    "##\n",
    "yy = pd.DataFrame( { 'PercentSalaryHike':y_test_hr } )\n",
    "ols_test_hr = yy.merge( X_test_hr, left_index = True, right_index = True )\n",
    "print( 'Testing Data Set:\\n\\n{}'.format( ols_test_hr.head() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution III.B.1\n",
    "\n",
    "[Return to Exercise III.B.1](#Exercise-III.B.1)\n",
    "\n",
    "Estimate an *OLS* model for *PercentSalaryHike* regressed on *Age*, *Department*, *TotalWorkingYears*, and *YearsAtCompany*.  Interpret the rsults.\n",
    "\n",
    "**Hint**: *Department* is categorical so you have to create dummies for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## OLS - Version 1\n",
    "##\n",
    "## ===> Step 1: Define a formula <===\n",
    "##\n",
    "formula = 'PercentSalaryHike ~ Age + C(Department) + TotalWorkingYears + YearsAtCompany'\n",
    "##\n",
    "## ===> Step 2: Instantiate the OLS model <===\n",
    "##\n",
    "mod = smf.ols( formula, data = ols_train_hr )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model <===\n",
    "##      Recommendation: number your models\n",
    "##\n",
    "reg01_hr = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model <===\n",
    "##\n",
    "print( reg01_hr.summary() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## OLS - Version 2: insignificant variables dropped\n",
    "##\n",
    "## ===> Step 1: Define a formula <===\n",
    "##\n",
    "formula = 'PercentSalaryHike ~ Age + YearsAtCompany'\n",
    "##\n",
    "## ===> Step 2: Instantiate the OLS model <===\n",
    "##\n",
    "mod = smf.ols( formula, data = ols_train_hr )\n",
    "##\n",
    "## ===> Step 3: Fit the instantiated model <===\n",
    "##      Recommendation: number your models\n",
    "##\n",
    "reg02_hr = mod.fit()\n",
    "##\n",
    "## ===> Step 4: Summarize the fitted model <===\n",
    "##\n",
    "print( reg02_hr.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution III.C.1\n",
    "\n",
    "[Return to Exercise III.C.1](#Exercise-III.C.1)\n",
    "\n",
    "The employee DataFrame is cross-sectional.  Create training and testing data sets using the default 1/4, 3/4 split.  You will soon model the attrition (*Attrition*) so use this as the *y* variable.  The *X* variables are *Age*, *Department*, *TotalWorkingYears*, *YearsAtCompany*, and *YearsSinceLastPromotion*.  Call the training set *train_hr* and the testing set *test_hr*, each with the prefix *logit*.  Use 1/4 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create the X and y data for splitting\n",
    "##\n",
    "y = df_hr[ 'Attrition' ]\n",
    "x = [ 'Age', 'Department', 'TotalWorkingYears', 'YearsAtCompany', 'YearsSinceLastPromotion' ]\n",
    "X = df_hr[ x ]\n",
    "##\n",
    "## Split the data: 1/4 and 3/4.\n",
    "##\n",
    "X_train_hr, X_test_hr, y_train_hr, y_test_hr = train_test_split( X, y, test_size = 0.25, \n",
    "                                                    random_state = 42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution III.C.2\n",
    "\n",
    "[Return to Exercise III.C.2](#Exercise-III.C.2)\n",
    "\n",
    "Merge the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the two training sets for convenience\n",
    "##\n",
    "yy = pd.DataFrame( { 'Attrition':y_train_hr } )\n",
    "logit_train_hr = yy.merge( X_train_hr, left_index = True, right_index = True )\n",
    "logit_train_hr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## \n",
    "## Merge the two testing sets for convenience\n",
    "##\n",
    "yy = pd.DataFrame( { 'Attrition':y_test_hr } )\n",
    "logit_test_hr = yy.merge( X_test_hr, left_index = True, right_index = True )\n",
    "logit_test_hr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution III.C.3\n",
    "\n",
    "[Return to Exercise III.C.3](#Exercise-III.C.3)\n",
    "\n",
    "Recode the Attrition variable in both data sets so that $Yes = 1$ and $No = 0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Recode using Numpy's select function\n",
    "##\n",
    "## Define labels for the recoded values\n",
    "##\n",
    "lbl = [ 0, 1 ]   ## 1 = Yes; 0 = No\n",
    "##\n",
    "## Specify the conditions for the recoding\n",
    "##\n",
    "conditions = [\n",
    "    ( logit_train_hr.Attrition == 'No' ),\n",
    "    ( logit_train_hr.Attrition == 'Yes' )\n",
    "]\n",
    "##\n",
    "## Do the recoding \n",
    "##\n",
    "logit_train_hr[ 'att' ] = np.select( conditions, lbl )\n",
    "##\n",
    "logit_train_hr[ 'att' ].value_counts( normalize = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## It is a good idea to check some records\n",
    "## to make sure the recoding is correct\n",
    "##\n",
    "logit_train_hr[ [ 'Attrition', 'att' ] ].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Recode using Numpy's select function\n",
    "##\n",
    "## Define labels for the recoded values\n",
    "##\n",
    "lbl = [ 0, 1 ]   ## 1 = Yes; 0 = No\n",
    "##\n",
    "## Specify the conditions for the recoding\n",
    "##\n",
    "conditions = [\n",
    "    ( logit_test_hr.Attrition == 'No' ),\n",
    "    ( logit_test_hr.Attrition == 'Yes' )\n",
    "]\n",
    "##\n",
    "## Do the recoding \n",
    "##\n",
    "logit_test_hr[ 'att' ] = np.select( conditions, lbl )\n",
    "##\n",
    "logit_test_hr[ 'att' ].value_counts( normalize = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution III.C.4\n",
    "\n",
    "[Return to Exercise III.C.4](#Exercise-III.C.4)\n",
    "\n",
    "Train a logit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Define a formula\n",
    "##\n",
    "formula = 'att ~ Age + C(Department) + TotalWorkingYears + YearsAtCompany + YearsSinceLastPromotion'\n",
    "##\n",
    "## Instantiate the logit model\n",
    "##\n",
    "mod = smf.logit( formula, data = logit_train_hr )\n",
    "##\n",
    "## Fit the instantiated model\n",
    "##\n",
    "logit01_hr = mod.fit()\n",
    "##\n",
    "## Summarize the fitted model\n",
    "##\n",
    "print( logit01_hr.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution IV.A.1\n",
    "\n",
    "[Return to Exercise IV.A.1](#Exercise-IV.A.1)\n",
    "\n",
    "Create a hierarchical clustering using the following variables:\n",
    "\n",
    "1. Age\n",
    "2. MonthlyIncome\n",
    "3. PercentSalaryHike\n",
    "4. TotalWorkingYears\n",
    "5. YearsAtCompany\n",
    "\n",
    "HINT 1: The first step is to standardize these five variables.\n",
    "HINT 2: The variable *TotalWorkingYears* has missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Make a list of the five numeric variables.\n",
    "##\n",
    "x = [ 'Age', 'MonthlyIncome', 'PercentSalaryHike', 'TotalWorkingYears', 'YearsAtCompany' ]\n",
    "##\n",
    "## Copy the five variables\n",
    "##\n",
    "df_hclusters_hr = df_hr[ x ].copy()\n",
    "##\n",
    "## Drop missing values\n",
    "##\n",
    "df_hclusters_hr.dropna( inplace = True )\n",
    "##\n",
    "## Standardize\n",
    "##\n",
    "df_hclusters_hr.loc[ :, x ] = StandardScaler().fit_transform( df_hclusters_hr[ x ] )\n",
    "df_hclusters_hr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Use Ward's minimum variance linkage method\n",
    "##\n",
    "ward_hr = shc.linkage( df_hclusters_hr, method = 'ward' )\n",
    "##\n",
    "## Plot a dendogram\n",
    "## WARNING:  this will take a minute\n",
    "##\n",
    "max_dist_hr = 60\n",
    "##\n",
    "plt.figure(figsize=(10, 7))  \n",
    "plt.title( 'Employee Clustering\\nHierarchical Clustering Dendrogram\\nWard\\'s Method' )\n",
    "plt.xlabel( 'Employees' )\n",
    "plt.ylabel( 'Distance' )\n",
    "dend_hr= shc.dendrogram( ward_hr )\n",
    "plt.axhline( y = max_dist_hr, c = 'black', ls = '-', lw = 1.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Identify the employees in each cluster\n",
    "## Consider any cluster grouping formed above 60\n",
    "##\n",
    "cluster_labels_hr = fcluster( ward_hr, max_dist_hr, criterion = 'distance' )\n",
    "df_hclusters_hr[ 'Cluster_Number' ] = cluster_labels_hr\n",
    "df_hclusters_hr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Bar chart of cluster sizes\n",
    "##\n",
    "y = df_hclusters_hr[ 'Cluster_Number' ].value_counts( normalize = True )\n",
    "ax = sns.barplot( y = y, x = [ 'Cluster ' + str( c ) for c in y.index ] )\n",
    "ax.set( title = 'Employee Hierarchical Clustering' );"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
